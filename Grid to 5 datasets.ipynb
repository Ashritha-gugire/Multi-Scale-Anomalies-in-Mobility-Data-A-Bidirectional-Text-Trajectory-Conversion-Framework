{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "!{sys.executable} -m pip install seaborn\n",
        "\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import gzip\n",
        "from collections import defaultdict\n",
        "import json"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: seaborn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.13.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.20 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from seaborn) (1.23.5)\nRequirement already satisfied: pandas>=1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from seaborn) (1.5.3)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from seaborn) (3.10.3)\nRequirement already satisfied: contourpy>=1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\nRequirement already satisfied: kiwisolver>=1.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\nRequirement already satisfied: pillow>=8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\nRequirement already satisfied: python-dateutil>=2.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1754327243861
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets():\n",
        "    \"\"\"\n",
        "    Load the two required datasets directly\n",
        "    \"\"\"\n",
        "    print(\"=== LOADING DATASETS ===\")\n",
        "    \n",
        "    # Load enhanced POI data from pickle file\n",
        "    import pickle\n",
        "    with open('complete_analysis_results.pkl', 'rb') as f:\n",
        "        results = pickle.load(f)\n",
        "    \n",
        "    enhanced_poi_df = results['enhanced_poi_df']\n",
        "    print(f\"Enhanced POI data loaded from pickle: {enhanced_poi_df.shape}\")\n",
        "    \n",
        "    # Load task2 dataset from Zenodo\n",
        "    print(\"Downloading task2 dataset...\")\n",
        "    task2_url = \"https://zenodo.org/records/10142719/files/yjmob100k-dataset2.csv.gz\"\n",
        "    response = requests.get(task2_url)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    with gzip.open(BytesIO(response.content), 'rt') as f:\n",
        "        task2_df = pd.read_csv(f)\n",
        "    \n",
        "    print(f\"Task2 dataset loaded: {task2_df.shape}\")\n",
        "    \n",
        "    return enhanced_poi_df, task2_df"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1754327243967
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA EXPLORATION\n",
        "# ============================================================================\n",
        "\n",
        "def explore_datasets(enhanced_poi_df, task2_df):\n",
        "    \"\"\"\n",
        "    Quick exploration of both datasets\n",
        "    \"\"\"\n",
        "    print(\"\\n=== DATASET EXPLORATION ===\")\n",
        "    \n",
        "    print(f\"POI Dataset: {enhanced_poi_df.shape}\")\n",
        "    print(f\"Columns: {list(enhanced_poi_df.columns)}\")\n",
        "    \n",
        "    print(f\"\\nTask2 Dataset: {task2_df.shape}\")\n",
        "    print(f\"Columns: {list(task2_df.columns)}\")\n",
        "    \n",
        "    print(f\"\\nUsers in task2: {task2_df['uid'].nunique()}\")\n",
        "    print(f\"POI coordinates: {len(enhanced_poi_df[['x', 'y']].drop_duplicates())}\")\n",
        "    print(f\"Task2 coordinates: {len(task2_df[['x', 'y']].drop_duplicates())}\")\n",
        "\n",
        "def filter_users(task2_df, n_users=10000):\n",
        "    \"\"\"\n",
        "    Filter to first N users\n",
        "    \"\"\"\n",
        "    unique_users = sorted(task2_df['uid'].unique())[:n_users]\n",
        "    filtered_df = task2_df[task2_df['uid'].isin(unique_users)].copy()\n",
        "    print(f\"\\nFiltered to {n_users} users: {filtered_df.shape}\")\n",
        "    return filtered_df\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1754327244074
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# POI PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_poi_data(enhanced_poi_df):\n",
        "    \"\"\"\n",
        "    Aggregate POI data per coordinate\n",
        "    \"\"\"\n",
        "    print(\"\\n=== PREPARING POI DATA ===\")\n",
        "    \n",
        "    # Check for multiple POIs per coordinate\n",
        "    coord_counts = enhanced_poi_df.groupby(['x', 'y']).size()\n",
        "    print(f\"Coordinates with multiple POIs: {(coord_counts > 1).sum()}\")\n",
        "    \n",
        "    # Aggregate POI data per coordinate\n",
        "    agg_dict = {}\n",
        "    if 'category' in enhanced_poi_df.columns:\n",
        "        agg_dict['category'] = lambda x: '|'.join(x.dropna().astype(str).unique())\n",
        "    if 'functional_group' in enhanced_poi_df.columns:\n",
        "        agg_dict['functional_group'] = lambda x: '|'.join(x.dropna().astype(str).unique())\n",
        "    if 'POI_count' in enhanced_poi_df.columns:\n",
        "        agg_dict['POI_count'] = 'sum'\n",
        "    if 'total_poi_count' in enhanced_poi_df.columns:\n",
        "        agg_dict['total_poi_count'] = 'sum'\n",
        "    if 'poi_proportion' in enhanced_poi_df.columns:\n",
        "        agg_dict['poi_proportion'] = 'mean'\n",
        "    \n",
        "    poi_agg = enhanced_poi_df.groupby(['x', 'y']).agg(agg_dict).reset_index()\n",
        "    \n",
        "    # Add diversity metrics\n",
        "    poi_agg['num_poi_types'] = enhanced_poi_df.groupby(['x', 'y'])['category'].nunique().values\n",
        "    poi_agg['num_functional_groups'] = enhanced_poi_df.groupby(['x', 'y'])['functional_group'].nunique().values\n",
        "    \n",
        "    print(f\"Aggregated POI data: {poi_agg.shape}\")\n",
        "    return poi_agg"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1754327244190
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MERGE AND ENHANCE\n",
        "# ============================================================================\n",
        "\n",
        "def merge_datasets(task2_df, poi_agg):\n",
        "    \"\"\"\n",
        "    Merge task2 data with POI data\n",
        "    \"\"\"\n",
        "    print(\"\\n=== MERGING DATASETS ===\")\n",
        "    \n",
        "    # Perform LEFT JOIN to keep all mobility data\n",
        "    merged_df = pd.merge(task2_df, poi_agg, on=['x', 'y'], how='left')\n",
        "    \n",
        "    print(f\"Original task2: {task2_df.shape}\")\n",
        "    print(f\"Merged dataset: {merged_df.shape}\")\n",
        "    \n",
        "    # Fill missing POI data\n",
        "    missing_poi_mask = merged_df['category'].isna()\n",
        "    merged_df.loc[missing_poi_mask, 'category'] = 'No_POI'\n",
        "    merged_df.loc[missing_poi_mask, 'functional_group'] = 'none'\n",
        "    \n",
        "    # Fill other POI columns\n",
        "    poi_columns = ['POI_count', 'total_poi_count', 'poi_proportion', 'num_poi_types', 'num_functional_groups']\n",
        "    for col in poi_columns:\n",
        "        if col in merged_df.columns:\n",
        "            merged_df.loc[missing_poi_mask, col] = 0\n",
        "    \n",
        "    print(f\"Records with POI data: {(~missing_poi_mask).sum()}\")\n",
        "    print(f\"Records without POI data: {missing_poi_mask.sum()}\")\n",
        "    \n",
        "    return merged_df\n",
        "\n",
        "def add_enhanced_features(merged_df):\n",
        "    \"\"\"\n",
        "    Add comprehensive time and spatial features\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ADDING ENHANCED FEATURES ===\")\n",
        "    \n",
        "    # Time features\n",
        "    merged_df['hour'] = (merged_df['t'] * 0.5).astype(int)\n",
        "    merged_df['minute'] = (merged_df['t'] * 30) % 60\n",
        "    \n",
        "    # Detailed time categorization\n",
        "    def get_detailed_time_period(hour):\n",
        "        if 5 <= hour < 7:\n",
        "            return 'Early_Morning'\n",
        "        elif 7 <= hour < 9:\n",
        "            return 'Morning_Rush'\n",
        "        elif 9 <= hour < 12:\n",
        "            return 'Late_Morning'\n",
        "        elif 12 <= hour < 14:\n",
        "            return 'Lunch_Time'\n",
        "        elif 14 <= hour < 17:\n",
        "            return 'Afternoon'\n",
        "        elif 17 <= hour < 19:\n",
        "            return 'Evening_Rush'\n",
        "        elif 19 <= hour < 22:\n",
        "            return 'Evening'\n",
        "        elif 22 <= hour < 24:\n",
        "            return 'Night'\n",
        "        else:\n",
        "            return 'Deep_Night'\n",
        "    \n",
        "    merged_df['time_period_detailed'] = merged_df['hour'].apply(get_detailed_time_period)\n",
        "    \n",
        "    # Day features\n",
        "    merged_df['day_of_week'] = merged_df['d'] % 7\n",
        "    merged_df['day_name'] = merged_df['day_of_week'].map({\n",
        "        0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
        "        4: 'Friday', 5: 'Saturday', 6: 'Sunday'\n",
        "    })\n",
        "    merged_df['is_weekend'] = merged_df['day_of_week'] >= 5\n",
        "    merged_df['is_weekday'] = merged_df['day_of_week'] < 5\n",
        "    \n",
        "    # Spatial features\n",
        "    center_x, center_y = merged_df['x'].mean(), merged_df['y'].mean()\n",
        "    merged_df['distance_from_center'] = np.sqrt(\n",
        "        (merged_df['x'] - center_x)**2 + (merged_df['y'] - center_y)**2\n",
        "    )\n",
        "    \n",
        "    # Spatial zones\n",
        "    merged_df['distance_quartile'] = pd.qcut(merged_df['distance_from_center'], \n",
        "                                           q=4, labels=['Central', 'Inner', 'Outer', 'Peripheral'])\n",
        "    \n",
        "    # Grid quadrants\n",
        "    merged_df['grid_quadrant'] = ((merged_df['x'] > merged_df['x'].median()).astype(int) * 2 + \n",
        "                                 (merged_df['y'] > merged_df['y'].median()).astype(int))\n",
        "    merged_df['grid_quadrant_name'] = merged_df['grid_quadrant'].map({\n",
        "        0: 'SW', 1: 'NW', 2: 'SE', 3: 'NE'\n",
        "    })\n",
        "    \n",
        "    # POI-based features\n",
        "    if 'total_poi_count' in merged_df.columns:\n",
        "        merged_df['poi_density_category'] = pd.cut(merged_df['total_poi_count'], \n",
        "                                                  bins=[0, 1, 10, 50, float('inf')],\n",
        "                                                  labels=['None', 'Low', 'Medium', 'High'])\n",
        "        \n",
        "        merged_df['poi_diversity_score'] = (merged_df['num_poi_types'] * merged_df['num_functional_groups'])\n",
        "        merged_df['location_attractiveness'] = (merged_df['total_poi_count'] * 0.7 + \n",
        "                                              merged_df['poi_diversity_score'] * 0.3)\n",
        "    \n",
        "    # Movement features\n",
        "    merged_df_sorted = merged_df.sort_values(['uid', 'd', 't'])\n",
        "    merged_df_sorted['prev_x'] = merged_df_sorted.groupby('uid')['x'].shift(1)\n",
        "    merged_df_sorted['prev_y'] = merged_df_sorted.groupby('uid')['y'].shift(1)\n",
        "    \n",
        "    merged_df_sorted['displacement'] = np.sqrt(\n",
        "        (merged_df_sorted['x'] - merged_df_sorted['prev_x'])**2 + \n",
        "        (merged_df_sorted['y'] - merged_df_sorted['prev_y'])**2\n",
        "    )\n",
        "    merged_df_sorted['displacement'] = merged_df_sorted['displacement'].fillna(0)\n",
        "    \n",
        "    print(f\"Enhanced dataset: {merged_df_sorted.shape}\")\n",
        "    return merged_df_sorted\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1754327244311
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CREATE COMPREHENSIVE SCIENTIFIC DATASETS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_poi_categories(enhanced_poi_df):\n",
        "    \"\"\"\n",
        "    Analyze POI categories for creating binary features\n",
        "    \"\"\"\n",
        "    print(\"\\n=== ANALYZING POI CATEGORIES ===\")\n",
        "    \n",
        "    # Get all unique categories\n",
        "    all_categories = []\n",
        "    for cat_string in enhanced_poi_df['category'].dropna():\n",
        "        if isinstance(cat_string, str) and '|' in cat_string:\n",
        "            all_categories.extend(cat_string.split('|'))\n",
        "        else:\n",
        "            all_categories.append(str(cat_string))\n",
        "    \n",
        "    unique_categories = pd.Series(all_categories).value_counts()\n",
        "    \n",
        "    # Get all unique functional groups\n",
        "    all_func_groups = []\n",
        "    for func_string in enhanced_poi_df['functional_group'].dropna():\n",
        "        if isinstance(func_string, str) and '|' in func_string:\n",
        "            all_func_groups.extend(func_string.split('|'))\n",
        "        else:\n",
        "            all_func_groups.append(str(func_string))\n",
        "    \n",
        "    unique_func_groups = pd.Series(all_func_groups).value_counts()\n",
        "    \n",
        "    print(f\"Total unique categories: {len(unique_categories)}\")\n",
        "    print(f\"Total unique functional groups: {len(unique_func_groups)}\")\n",
        "    \n",
        "    return unique_categories, unique_func_groups\n",
        "\n",
        "def create_comprehensive_scientific_datasets(base_merged_df, unique_categories, unique_func_groups):\n",
        "    \"\"\"\n",
        "    Create all 5 comprehensive scientific datasets\n",
        "    \"\"\"\n",
        "    print(\"\\n=== CREATING COMPREHENSIVE SCIENTIFIC DATASETS ===\")\n",
        "    \n",
        "    # DATASET 1: Base mobility data with primary POI information\n",
        "    print(\"\\n1. Creating BASE MOBILITY dataset...\")\n",
        "    base_df = base_merged_df.copy()\n",
        "    \n",
        "    # Extract primary (first) category and functional group\n",
        "    base_df['primary_category'] = base_df['category'].astype(str).str.split('|').str[0]\n",
        "    base_df['primary_functional_group'] = base_df['functional_group'].astype(str).str.split('|').str[0]\n",
        "    \n",
        "    # Select base columns\n",
        "    base_columns = ['uid', 'd', 't', 'x', 'y', \n",
        "                   'primary_category', 'primary_functional_group',\n",
        "                   'total_poi_count', 'poi_proportion', \n",
        "                   'num_poi_types', 'num_functional_groups',\n",
        "                   'hour', 'time_period_detailed', 'day_of_week', 'day_name',\n",
        "                   'is_weekend', 'is_weekday', 'distance_from_center',\n",
        "                   'distance_quartile', 'grid_quadrant_name', 'displacement']\n",
        "    \n",
        "    # Filter to available columns\n",
        "    available_base_columns = [col for col in base_columns if col in base_df.columns]\n",
        "    base_mobility_df = base_df[available_base_columns].copy()\n",
        "    \n",
        "    # Rename for clarity\n",
        "    base_mobility_df = base_mobility_df.rename(columns={\n",
        "        'uid': 'user_id',\n",
        "        'd': 'day', \n",
        "        't': 'time_slot',\n",
        "        'x': 'grid_x',\n",
        "        'y': 'grid_y',\n",
        "        'primary_category': 'location_category',\n",
        "        'primary_functional_group': 'location_function',\n",
        "        'total_poi_count': 'poi_density',\n",
        "        'num_poi_types': 'category_diversity',\n",
        "        'num_functional_groups': 'functional_diversity'\n",
        "    })\n",
        "    \n",
        "    print(f\"   Base mobility dataset shape: {base_mobility_df.shape}\")\n",
        "    \n",
        "    # DATASET 2: POI presence matrix\n",
        "    print(\"\\n2. Creating POI PRESENCE MATRIX...\")\n",
        "    \n",
        "    # Get top categories for binary encoding (limit to manageable number)\n",
        "    top_categories = unique_categories.head(20).index.tolist()\n",
        "    \n",
        "    poi_matrix_df = base_mobility_df.copy()\n",
        "    \n",
        "    # Create binary columns for each top category\n",
        "    for category in top_categories:\n",
        "        col_name = f\"has_{category.replace(' ', '_').replace('|', '_').lower()}\"\n",
        "        poi_matrix_df[col_name] = base_merged_df['category'].astype(str).str.contains(category, na=False).astype(int)\n",
        "    \n",
        "    # Create binary columns for functional groups\n",
        "    for func_group in unique_func_groups.index:\n",
        "        col_name = f\"has_function_{func_group.replace(' ', '_').replace('|', '_').lower()}\"\n",
        "        poi_matrix_df[col_name] = base_merged_df['functional_group'].astype(str).str.contains(func_group, na=False).astype(int)\n",
        "    \n",
        "    print(f\"   POI presence matrix shape: {poi_matrix_df.shape}\")\n",
        "    print(f\"   Added {len(top_categories)} category binary features\")\n",
        "    print(f\"   Added {len(unique_func_groups)} functional binary features\")\n",
        "    \n",
        "    # DATASET 3: Functional group focused dataset\n",
        "    print(\"\\n3. Creating FUNCTIONAL GROUP dataset...\")\n",
        "    \n",
        "    functional_df = base_mobility_df.copy()\n",
        "    \n",
        "    # Create functional group hierarchy\n",
        "    def categorize_functional_group(func_group):\n",
        "        if pd.isna(func_group) or func_group == 'none':\n",
        "            return 'No_Function'\n",
        "        func_group_str = str(func_group).lower()\n",
        "        if 'food' in func_group_str or 'dining' in func_group_str:\n",
        "            return 'Food_Services'\n",
        "        elif 'shopping' in func_group_str or 'retail' in func_group_str:\n",
        "            return 'Retail_Shopping'\n",
        "        elif 'transport' in func_group_str:\n",
        "            return 'Transportation'\n",
        "        elif 'education' in func_group_str:\n",
        "            return 'Education'\n",
        "        elif 'health' in func_group_str or 'medical' in func_group_str:\n",
        "            return 'Healthcare'\n",
        "        elif 'entertainment' in func_group_str or 'recreation' in func_group_str:\n",
        "            return 'Entertainment'\n",
        "        elif 'service' in func_group_str:\n",
        "            return 'Services'\n",
        "        elif 'business' in func_group_str:\n",
        "            return 'Business'\n",
        "        elif 'religious' in func_group_str:\n",
        "            return 'Religious'\n",
        "        else:\n",
        "            return 'Other'\n",
        "    \n",
        "    functional_df['functional_category'] = functional_df['location_function'].apply(categorize_functional_group)\n",
        "    \n",
        "    print(f\"   Functional group dataset shape: {functional_df.shape}\")\n",
        "    func_dist = functional_df['functional_category'].value_counts()\n",
        "    print(f\"   Functional categories distribution:\")\n",
        "    for func, count in func_dist.items():\n",
        "        print(f\"     {func}: {count:,} ({count/len(functional_df)*100:.1f}%)\")\n",
        "    \n",
        "    return base_mobility_df, poi_matrix_df, functional_df\n",
        "\n",
        "def create_user_level_features(base_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive user-level features\n",
        "    \"\"\"\n",
        "    print(\"\\n=== CREATING USER-LEVEL FEATURES ===\")\n",
        "    \n",
        "    # Calculate user-level statistics\n",
        "    agg_dict = {\n",
        "        'day': 'nunique',\n",
        "        'time_slot': 'count',\n",
        "        'grid_x': ['std', 'nunique', 'min', 'max'],\n",
        "        'grid_y': ['std', 'nunique', 'min', 'max'],\n",
        "        'distance_from_center': ['mean', 'std', 'max'],\n",
        "        'is_weekend': 'mean'\n",
        "    }\n",
        "    \n",
        "    # Add POI-related columns if available\n",
        "    if 'poi_density' in base_df.columns:\n",
        "        agg_dict['poi_density'] = ['mean', 'std', 'max']\n",
        "    if 'category_diversity' in base_df.columns:\n",
        "        agg_dict['category_diversity'] = ['mean', 'std']\n",
        "    if 'functional_diversity' in base_df.columns:\n",
        "        agg_dict['functional_diversity'] = ['mean', 'std']\n",
        "    if 'displacement' in base_df.columns:\n",
        "        agg_dict['displacement'] = ['mean', 'std', 'sum']\n",
        "    \n",
        "    user_features = base_df.groupby('user_id').agg(agg_dict).round(3)\n",
        "    \n",
        "    # Flatten column names\n",
        "    user_features.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col \n",
        "                           for col in user_features.columns.values]\n",
        "    \n",
        "    # Rename for clarity\n",
        "    column_mapping = {\n",
        "        'day_nunique': 'active_days',\n",
        "        'time_slot_count': 'total_records',\n",
        "        'grid_x_std': 'mobility_x_std',\n",
        "        'grid_x_nunique': 'unique_x_locations',\n",
        "        'grid_y_std': 'mobility_y_std', \n",
        "        'grid_y_nunique': 'unique_y_locations',\n",
        "        'distance_from_center_mean': 'avg_distance_from_center',\n",
        "        'distance_from_center_std': 'spatial_range_variability',\n",
        "        'distance_from_center_max': 'max_distance_from_center',\n",
        "        'is_weekend_mean': 'weekend_activity_ratio'\n",
        "    }\n",
        "    \n",
        "    # Add POI-related mappings if columns exist\n",
        "    if 'poi_density_mean' in user_features.columns:\n",
        "        column_mapping.update({\n",
        "            'poi_density_mean': 'avg_poi_density',\n",
        "            'poi_density_std': 'poi_density_variability'\n",
        "        })\n",
        "    \n",
        "    if 'displacement_mean' in user_features.columns:\n",
        "        column_mapping.update({\n",
        "            'displacement_mean': 'avg_displacement',\n",
        "            'displacement_std': 'displacement_variability',\n",
        "            'displacement_sum': 'total_displacement'\n",
        "        })\n",
        "    \n",
        "    user_features = user_features.rename(columns=column_mapping)\n",
        "    \n",
        "    # Calculate additional metrics\n",
        "    user_features['mobility_radius'] = np.sqrt(\n",
        "        user_features['mobility_x_std']**2 + user_features['mobility_y_std']**2\n",
        "    )\n",
        "    \n",
        "    user_features['spatial_coverage'] = (\n",
        "        user_features['unique_x_locations'] * user_features['unique_y_locations']\n",
        "    )\n",
        "    \n",
        "    user_features['activity_intensity'] = (\n",
        "        user_features['total_records'] / user_features['active_days']\n",
        "    )\n",
        "    \n",
        "    print(f\"User-level features dataset shape: {user_features.shape}\")\n",
        "    \n",
        "    return user_features\n",
        "\n",
        "def create_location_level_features(base_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive location-level features\n",
        "    \"\"\"\n",
        "    print(\"\\n=== CREATING LOCATION-LEVEL FEATURES ===\")\n",
        "    \n",
        "    # Calculate location-level statistics\n",
        "    agg_dict = {\n",
        "        'user_id': 'nunique',\n",
        "        'day': 'nunique', \n",
        "        'time_slot': 'count',\n",
        "        'location_category': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown',\n",
        "        'location_function': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown',\n",
        "        'distance_from_center': 'first',\n",
        "        'is_weekend': 'mean'\n",
        "    }\n",
        "    \n",
        "    # Add POI-related columns if available\n",
        "    if 'poi_density' in base_df.columns:\n",
        "        agg_dict['poi_density'] = 'first'\n",
        "    if 'category_diversity' in base_df.columns:\n",
        "        agg_dict['category_diversity'] = 'first'\n",
        "    if 'functional_diversity' in base_df.columns:\n",
        "        agg_dict['functional_diversity'] = 'first'\n",
        "    if 'hour' in base_df.columns:\n",
        "        agg_dict['hour'] = ['min', 'max', 'nunique']\n",
        "    if 'displacement' in base_df.columns:\n",
        "        agg_dict['displacement'] = 'mean'\n",
        "    \n",
        "    location_features = base_df.groupby(['grid_x', 'grid_y']).agg(agg_dict).round(3)\n",
        "    \n",
        "    # Flatten column names\n",
        "    location_features.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col \n",
        "                               for col in location_features.columns.values]\n",
        "    \n",
        "    # Rename for clarity\n",
        "    location_features = location_features.rename(columns={\n",
        "        'user_id_nunique': 'unique_visitors',\n",
        "        'day_nunique': 'active_days',\n",
        "        'time_slot_count': 'total_visits',\n",
        "        'is_weekend_mean': 'weekend_visit_ratio'\n",
        "    })\n",
        "    \n",
        "    # Add hour-related columns if available\n",
        "    if 'hour_min' in location_features.columns:\n",
        "        location_features = location_features.rename(columns={\n",
        "            'hour_min': 'earliest_visit_hour',\n",
        "            'hour_max': 'latest_visit_hour',\n",
        "            'hour_nunique': 'active_hours'\n",
        "        })\n",
        "        \n",
        "        location_features['temporal_span'] = (\n",
        "            location_features['latest_visit_hour'] - location_features['earliest_visit_hour']\n",
        "        )\n",
        "    \n",
        "    # Calculate additional metrics\n",
        "    location_features['popularity_score'] = (\n",
        "        location_features['unique_visitors'] * location_features['total_visits']\n",
        "    )\n",
        "    \n",
        "    print(f\"Location-level features dataset shape: {location_features.shape}\")\n",
        "    \n",
        "    return location_features"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1754327244463
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXPORT ALL DATASETS\n",
        "# ============================================================================\n",
        "\n",
        "def export_all_scientific_datasets(base_mobility_df, poi_matrix_df, functional_df, \n",
        "                                  user_features_df, location_features_df):\n",
        "    \"\"\"\n",
        "    Export all 5 comprehensive scientific datasets\n",
        "    \"\"\"\n",
        "    print(\"\\n=== EXPORTING ALL SCIENTIFIC DATASETS ===\")\n",
        "    \n",
        "    # Export datasets\n",
        "    datasets = {\n",
        "        'scientific_mobility_base_dataset.csv': base_mobility_df,\n",
        "        'scientific_mobility_poi_matrix_dataset.csv': poi_matrix_df,\n",
        "        'scientific_mobility_functional_dataset.csv': functional_df,\n",
        "        'scientific_user_profiles_dataset.csv': user_features_df,\n",
        "        'scientific_location_profiles_dataset.csv': location_features_df\n",
        "    }\n",
        "    \n",
        "    exported_files = []\n",
        "    for filename, dataset in datasets.items():\n",
        "        dataset.to_csv(filename, index=False)\n",
        "        exported_files.append(filename)\n",
        "        print(f\"Exported: {filename} ({dataset.shape[0]:,} rows, {dataset.shape[1]} cols)\")\n",
        "    \n",
        "    # Create comprehensive documentation\n",
        "    with open('comprehensive_datasets_documentation.md', 'w') as f:\n",
        "        f.write(\"# Comprehensive Scientific Mobility Datasets Documentation\\n\\n\")\n",
        "        f.write(\"This package contains 5 comprehensive datasets for scientific mobility analysis.\\n\\n\")\n",
        "        \n",
        "        f.write(\"## Dataset Overview\\n\\n\")\n",
        "        \n",
        "        descriptions = {\n",
        "            'scientific_mobility_base_dataset.csv': 'Core mobility data with primary POI information and temporal/spatial features',\n",
        "            'scientific_mobility_poi_matrix_dataset.csv': 'Mobility data with binary POI presence indicators for detailed analysis',\n",
        "            'scientific_mobility_functional_dataset.csv': 'Mobility data with hierarchical functional group categorization',\n",
        "            'scientific_user_profiles_dataset.csv': 'Aggregated user-level features and mobility patterns',\n",
        "            'scientific_location_profiles_dataset.csv': 'Aggregated location-level features and visit patterns'\n",
        "        }\n",
        "        \n",
        "        for filename, description in descriptions.items():\n",
        "            dataset = datasets[filename]\n",
        "            f.write(f\"### {filename}\\n\")\n",
        "            f.write(f\"- **Description**: {description}\\n\")\n",
        "            f.write(f\"- **Dimensions**: {dataset.shape[0]:,} rows × {dataset.shape[1]} columns\\n\")\n",
        "            if 'user_id' in dataset.columns:\n",
        "                f.write(f\"- **Users**: {dataset['user_id'].nunique()}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "        \n",
        "        f.write(\"## Research Applications\\n\\n\")\n",
        "        f.write(\"These datasets are designed for:\\n\")\n",
        "        f.write(\"- **Anomaly Detection**: Identify unusual mobility patterns\\n\")\n",
        "        f.write(\"- **Temporal Analysis**: Study time-based mobility patterns\\n\")\n",
        "        f.write(\"- **Spatial Analysis**: Analyze location-based behaviors\\n\")\n",
        "        f.write(\"- **User Profiling**: Characterize different mobility types\\n\")\n",
        "        f.write(\"- **Location Profiling**: Understand place characteristics\\n\")\n",
        "        f.write(\"- **Multi-scale Analysis**: From individual to aggregate patterns\\n\\n\")\n",
        "    \n",
        "    # Create summary metadata\n",
        "    metadata = {\n",
        "        'created_date': datetime.now().isoformat(),\n",
        "        'datasets': {\n",
        "            filename: {\n",
        "                'shape': dataset.shape,\n",
        "                'columns': list(dataset.columns),\n",
        "                'users': dataset['user_id'].nunique() if 'user_id' in dataset.columns else 'N/A'\n",
        "            }\n",
        "            for filename, dataset in datasets.items()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open('comprehensive_datasets_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(\"Documentation: comprehensive_datasets_documentation.md\")\n",
        "    print(\"Metadata: comprehensive_datasets_metadata.json\")\n",
        "    \n",
        "    return exported_files"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1754327244597
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def run_comprehensive_grid_poi_merge():\n",
        "    \"\"\"\n",
        "    Execute the complete comprehensive merge process creating all 5 datasets\n",
        "    \"\"\"\n",
        "    print(\"COMPREHENSIVE GRID-POI DATASET MERGE FOR IEEE RESEARCH\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load datasets\n",
        "    enhanced_poi_df, task2_df = load_datasets()\n",
        "    \n",
        "    # Explore\n",
        "    explore_datasets(enhanced_poi_df, task2_df)\n",
        "    \n",
        "    # Filter users\n",
        "    filtered_task2_df = filter_users(task2_df, n_users=10000)\n",
        "    \n",
        "    # Prepare POI data\n",
        "    poi_agg = prepare_poi_data(enhanced_poi_df)\n",
        "    \n",
        "    # Merge datasets\n",
        "    merged_df = merge_datasets(filtered_task2_df, poi_agg)\n",
        "    \n",
        "    # Add comprehensive features\n",
        "    enhanced_df = add_enhanced_features(merged_df)\n",
        "    \n",
        "    # Analyze POI categories for binary features\n",
        "    unique_categories, unique_func_groups = analyze_poi_categories(enhanced_poi_df)\n",
        "    \n",
        "    # Create all 5 comprehensive scientific datasets\n",
        "    base_mobility_df, poi_matrix_df, functional_df = create_comprehensive_scientific_datasets(\n",
        "        enhanced_df, unique_categories, unique_func_groups)\n",
        "    \n",
        "    # Create user and location profiles\n",
        "    user_features_df = create_user_level_features(base_mobility_df)\n",
        "    location_features_df = create_location_level_features(base_mobility_df)\n",
        "    \n",
        "    # Export all datasets\n",
        "    exported_files = export_all_scientific_datasets(\n",
        "        base_mobility_df, poi_matrix_df, functional_df, \n",
        "        user_features_df, location_features_df)\n",
        "    \n",
        "    print(f\"\\nCOMPREHENSIVE MERGE COMPLETE - ALL {len(exported_files)} DATASETS READY FOR IEEE RESEARCH\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return {\n",
        "        'base_mobility': base_mobility_df,\n",
        "        'poi_matrix': poi_matrix_df,\n",
        "        'functional': functional_df,\n",
        "        'user_profiles': user_features_df,\n",
        "        'location_profiles': location_features_df,\n",
        "        'exported_files': exported_files\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1754327244706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = run_comprehensive_grid_poi_merge()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "COMPREHENSIVE GRID-POI DATASET MERGE FOR IEEE RESEARCH\n============================================================\n=== LOADING DATASETS ===\nEnhanced POI data loaded from pickle: (221159, 9)\nDownloading task2 dataset...\nTask2 dataset loaded: (29389749, 5)\n\n=== DATASET EXPLORATION ===\nPOI Dataset: (221159, 9)\nColumns: ['x', 'y', 'POIcategory', 'POI_count', 'id', 'category', 'functional_group', 'total_poi_count', 'poi_proportion']\n\nTask2 Dataset: (29389749, 5)\nColumns: ['uid', 'd', 't', 'x', 'y']\n\nUsers in task2: 25000\nPOI coordinates: 20146\nTask2 coordinates: 30277\n\nFiltered to 10000 users: (12247358, 5)\n\n=== PREPARING POI DATA ===\nCoordinates with multiple POIs: 17136\nAggregated POI data: (20146, 9)\n\n=== MERGING DATASETS ===\nOriginal task2: (12247358, 5)\nMerged dataset: (12247358, 12)\nRecords with POI data: 11896914\nRecords without POI data: 350444\n\n=== ADDING ENHANCED FEATURES ===\nEnhanced dataset: (12247358, 29)\n\n=== ANALYZING POI CATEGORIES ===\nTotal unique categories: 84\nTotal unique functional groups: 10\n\n=== CREATING COMPREHENSIVE SCIENTIFIC DATASETS ===\n\n1. Creating BASE MOBILITY dataset...\n   Base mobility dataset shape: (12247358, 21)\n\n2. Creating POI PRESENCE MATRIX...\n   POI presence matrix shape: (12247358, 51)\n   Added 20 category binary features\n   Added 10 functional binary features\n\n3. Creating FUNCTIONAL GROUP dataset...\n   Functional group dataset shape: (12247358, 22)\n   Functional categories distribution:\n     Food_Services: 7,494,892 (61.2%)\n     Retail_Shopping: 2,440,440 (19.9%)\n     Transportation: 755,084 (6.2%)\n     Services: 442,150 (3.6%)\n     No_Function: 350,444 (2.9%)\n     Entertainment: 275,504 (2.2%)\n     Religious: 136,366 (1.1%)\n     Other: 123,265 (1.0%)\n     Business: 118,229 (1.0%)\n     Education: 110,984 (0.9%)\n\n=== CREATING USER-LEVEL FEATURES ===\nUser-level features dataset shape: (10000, 27)\n\n=== CREATING LOCATION-LEVEL FEATURES ===\nLocation-level features dataset shape: (27453, 16)\n\n=== EXPORTING ALL SCIENTIFIC DATASETS ===\nExported: scientific_mobility_base_dataset.csv (12,247,358 rows, 21 cols)\nExported: scientific_mobility_poi_matrix_dataset.csv (12,247,358 rows, 51 cols)\nExported: scientific_mobility_functional_dataset.csv (12,247,358 rows, 22 cols)\nExported: scientific_user_profiles_dataset.csv (10,000 rows, 27 cols)\nExported: scientific_location_profiles_dataset.csv (27,453 rows, 16 cols)\nDocumentation: comprehensive_datasets_documentation.md\nMetadata: comprehensive_datasets_metadata.json\n\nCOMPREHENSIVE MERGE COMPLETE - ALL 5 DATASETS READY FOR IEEE RESEARCH\n============================================================\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1754328001836
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scientific_mobility_base_dataset.head()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scientific_mobility_base_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscientific_mobility_base_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scientific_mobility_base_dataset' is not defined"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1754345713694
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}