{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1754496885108
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-06 16:19:40,390 - INFO - Starting mobility text generation continuation processing\n",
            "2025-08-06 16:19:40,391 - INFO - Target: IEEE Transactions on Knowledge and Data Engineering (TKDE)\n",
            "2025-08-06 16:20:36,511 - INFO - Loaded base dataset: (12247358, 21)\n",
            "2025-08-06 16:20:36,511 - INFO - Loaded functional dataset: (12247358, 22)\n",
            "2025-08-06 16:20:42,507 - INFO - Merged dataset shape: (12247358, 22)\n",
            "2025-08-06 16:20:42,508 - INFO - Columns available: ['user_id', 'day', 'time_slot', 'grid_x', 'grid_y', 'location_category', 'location_function', 'poi_density', 'poi_proportion', 'category_diversity', 'functional_diversity', 'hour', 'time_period_detailed', 'day_of_week', 'day_name', 'is_weekend', 'is_weekday', 'distance_from_center', 'distance_quartile', 'grid_quadrant_name', 'displacement', 'functional_category']\n",
            "2025-08-06 16:20:42,816 - INFO - Processing 12,247,358 mobility records\n",
            "2025-08-06 16:20:42,818 - INFO - Processing medium narrative style\n",
            "2025-08-06 16:20:42,818 - INFO - Processing medium style for up to 1000 users in batches of 50\n",
            "2025-08-06 16:20:42,873 - INFO - Found 10000 total users, processing 1000\n",
            "2025-08-06 16:20:42,874 - INFO - Processing batch 1: users 1-50\n",
            "2025-08-06 16:20:54,294 - INFO - Batch 1 completed\n",
            "2025-08-06 16:20:54,295 - INFO - Processing batch 2: users 51-100\n",
            "2025-08-06 16:21:03,240 - INFO - Batch 2 completed\n",
            "2025-08-06 16:21:03,241 - INFO - Processing batch 3: users 101-150\n",
            "2025-08-06 16:21:11,357 - INFO - Batch 3 completed\n",
            "2025-08-06 16:21:11,358 - INFO - Processing batch 4: users 151-200\n",
            "2025-08-06 16:21:20,205 - INFO - Batch 4 completed\n",
            "2025-08-06 16:21:20,205 - INFO - Processing batch 5: users 201-250\n",
            "2025-08-06 16:21:28,348 - INFO - Batch 5 completed\n",
            "2025-08-06 16:21:28,349 - INFO - Processing batch 6: users 251-300\n",
            "2025-08-06 16:21:36,813 - INFO - Batch 6 completed\n",
            "2025-08-06 16:21:36,814 - INFO - Processing batch 7: users 301-350\n",
            "2025-08-06 16:21:45,141 - INFO - Batch 7 completed\n",
            "2025-08-06 16:21:45,142 - INFO - Processing batch 8: users 351-400\n",
            "2025-08-06 16:21:53,452 - INFO - Batch 8 completed\n",
            "2025-08-06 16:21:53,453 - INFO - Processing batch 9: users 401-450\n",
            "2025-08-06 16:22:02,015 - INFO - Batch 9 completed\n",
            "2025-08-06 16:22:02,016 - INFO - Processing batch 10: users 451-500\n",
            "2025-08-06 16:22:10,524 - INFO - Batch 10 completed\n",
            "2025-08-06 16:22:10,524 - INFO - Processing batch 11: users 501-550\n",
            "2025-08-06 16:22:19,664 - INFO - Batch 11 completed\n",
            "2025-08-06 16:22:19,665 - INFO - Processing batch 12: users 551-600\n",
            "2025-08-06 16:22:28,735 - INFO - Batch 12 completed\n",
            "2025-08-06 16:22:28,736 - INFO - Processing batch 13: users 601-650\n",
            "2025-08-06 16:22:38,114 - INFO - Batch 13 completed\n",
            "2025-08-06 16:22:38,115 - INFO - Processing batch 14: users 651-700\n",
            "2025-08-06 16:22:47,022 - INFO - Batch 14 completed\n",
            "2025-08-06 16:22:47,023 - INFO - Processing batch 15: users 701-750\n",
            "2025-08-06 16:22:56,175 - INFO - Batch 15 completed\n",
            "2025-08-06 16:22:56,176 - INFO - Processing batch 16: users 751-800\n",
            "2025-08-06 16:23:05,466 - INFO - Batch 16 completed\n",
            "2025-08-06 16:23:05,466 - INFO - Processing batch 17: users 801-850\n",
            "2025-08-06 16:23:14,269 - INFO - Batch 17 completed\n",
            "2025-08-06 16:23:14,269 - INFO - Processing batch 18: users 851-900\n",
            "2025-08-06 16:23:23,452 - INFO - Batch 18 completed\n",
            "2025-08-06 16:23:23,453 - INFO - Processing batch 19: users 901-950\n",
            "2025-08-06 16:23:32,215 - INFO - Batch 19 completed\n",
            "2025-08-06 16:23:32,216 - INFO - Processing batch 20: users 951-1000\n",
            "2025-08-06 16:23:41,044 - INFO - Batch 20 completed\n",
            "2025-08-06 16:23:41,109 - INFO - Saving medium style files\n",
            "2025-08-06 16:25:42,512 - INFO - Saved 1000 users for normal_period\n",
            "2025-08-06 16:26:50,675 - INFO - Saved 1000 users for emergency_period\n",
            "2025-08-06 16:26:54,973 - INFO - Medium style processing completed\n",
            "2025-08-06 16:26:54,974 - INFO - Processing detailed narrative style\n",
            "2025-08-06 16:26:54,975 - INFO - Processing detailed style for up to 1000 users in batches of 50\n",
            "2025-08-06 16:26:55,030 - INFO - Found 10000 total users, processing 1000\n",
            "2025-08-06 16:26:55,031 - INFO - Processing batch 1: users 1-50\n",
            "2025-08-06 16:27:03,965 - INFO - Batch 1 completed\n",
            "2025-08-06 16:27:03,965 - INFO - Processing batch 2: users 51-100\n",
            "2025-08-06 16:27:12,788 - INFO - Batch 2 completed\n",
            "2025-08-06 16:27:12,789 - INFO - Processing batch 3: users 101-150\n",
            "2025-08-06 16:27:20,875 - INFO - Batch 3 completed\n",
            "2025-08-06 16:27:20,876 - INFO - Processing batch 4: users 151-200\n",
            "2025-08-06 16:27:29,729 - INFO - Batch 4 completed\n",
            "2025-08-06 16:27:29,730 - INFO - Processing batch 5: users 201-250\n",
            "2025-08-06 16:27:37,858 - INFO - Batch 5 completed\n",
            "2025-08-06 16:27:37,859 - INFO - Processing batch 6: users 251-300\n",
            "2025-08-06 16:27:46,306 - INFO - Batch 6 completed\n",
            "2025-08-06 16:27:46,307 - INFO - Processing batch 7: users 301-350\n",
            "2025-08-06 16:27:54,617 - INFO - Batch 7 completed\n",
            "2025-08-06 16:27:54,618 - INFO - Processing batch 8: users 351-400\n",
            "2025-08-06 16:28:02,825 - INFO - Batch 8 completed\n",
            "2025-08-06 16:28:02,826 - INFO - Processing batch 9: users 401-450\n",
            "2025-08-06 16:28:11,346 - INFO - Batch 9 completed\n",
            "2025-08-06 16:28:11,347 - INFO - Processing batch 10: users 451-500\n",
            "2025-08-06 16:28:19,838 - INFO - Batch 10 completed\n",
            "2025-08-06 16:28:19,839 - INFO - Processing batch 11: users 501-550\n",
            "2025-08-06 16:28:29,017 - INFO - Batch 11 completed\n",
            "2025-08-06 16:28:29,018 - INFO - Processing batch 12: users 551-600\n",
            "2025-08-06 16:28:38,064 - INFO - Batch 12 completed\n",
            "2025-08-06 16:28:38,065 - INFO - Processing batch 13: users 601-650\n",
            "2025-08-06 16:28:47,415 - INFO - Batch 13 completed\n",
            "2025-08-06 16:28:47,416 - INFO - Processing batch 14: users 651-700\n",
            "2025-08-06 16:28:56,353 - INFO - Batch 14 completed\n",
            "2025-08-06 16:28:56,354 - INFO - Processing batch 15: users 701-750\n",
            "2025-08-06 16:29:05,458 - INFO - Batch 15 completed\n",
            "2025-08-06 16:29:05,459 - INFO - Processing batch 16: users 751-800\n",
            "2025-08-06 16:29:14,739 - INFO - Batch 16 completed\n",
            "2025-08-06 16:29:14,740 - INFO - Processing batch 17: users 801-850\n",
            "2025-08-06 16:29:23,492 - INFO - Batch 17 completed\n",
            "2025-08-06 16:29:23,493 - INFO - Processing batch 18: users 851-900\n",
            "2025-08-06 16:29:32,705 - INFO - Batch 18 completed\n",
            "2025-08-06 16:29:32,706 - INFO - Processing batch 19: users 901-950\n",
            "2025-08-06 16:29:41,470 - INFO - Batch 19 completed\n",
            "2025-08-06 16:29:41,471 - INFO - Processing batch 20: users 951-1000\n",
            "2025-08-06 16:29:50,259 - INFO - Batch 20 completed\n",
            "2025-08-06 16:29:50,322 - INFO - Saving detailed style files\n",
            "2025-08-06 16:31:52,464 - INFO - Saved 1000 users for normal_period\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MobilityTextProcessor:\n",
        "    \"\"\"\n",
        "    A memory-efficient processor for generating natural language descriptions\n",
        "    from mobility trajectory data, designed for large-scale analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_users=1000, batch_size=50):\n",
        "        self.max_users = max_users\n",
        "        self.batch_size = batch_size\n",
        "        self.base_path = 'UserMobilityTexts'\n",
        "        \n",
        "        # Activity mapping for functional categories\n",
        "        self.activity_mapping = {\n",
        "            'food_dining': 'dining',\n",
        "            'retail_shopping': 'shopping', \n",
        "            'entertainment': 'entertainment',\n",
        "            'professional': 'work',\n",
        "            'healthcare': 'medical appointment',\n",
        "            'education': 'learning',\n",
        "            'transportation': 'commuting',\n",
        "            'residential': 'home activities'\n",
        "        }\n",
        "        \n",
        "    def load_mobility_datasets(self):\n",
        "        \"\"\"Load all mobility datasets and merge them - works with your actual files\"\"\"\n",
        "        try:\n",
        "            # Load your actual CSV files\n",
        "            base_df = pd.read_csv(\"scientific_mobility_base_dataset.csv\")\n",
        "            functional_df = pd.read_csv(\"scientific_mobility_functional_dataset.csv\")\n",
        "            logger.info(f\"Loaded base dataset: {base_df.shape}\")\n",
        "            logger.info(f\"Loaded functional dataset: {functional_df.shape}\")\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"CSV files not found: {e}\")\n",
        "            logger.info(\"Please ensure these files are uploaded:\")\n",
        "            logger.info(\"- scientific_mobility_base_dataset.csv\")\n",
        "            logger.info(\"- scientific_mobility_functional_dataset.csv\")\n",
        "            return None\n",
        "        \n",
        "        # Merge base and functional datasets\n",
        "        merged_df = base_df.merge(\n",
        "            functional_df[['user_id', 'day', 'time_slot', 'functional_category']], \n",
        "            on=['user_id', 'day', 'time_slot'], \n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Merged dataset shape: {merged_df.shape}\")\n",
        "        logger.info(f\"Columns available: {list(merged_df.columns)}\")\n",
        "        \n",
        "        return merged_df\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_period_type(day):\n",
        "        \"\"\"Classify day into period type for temporal analysis.\"\"\"\n",
        "        if day <= 60:\n",
        "            return 'normal'\n",
        "        elif day <= 75:\n",
        "            return 'emergency'\n",
        "        else:\n",
        "            return 'post_emergency'\n",
        "    \n",
        "    def generate_time_context(self, row):\n",
        "        \"\"\"Generate natural language time context from available data.\"\"\"\n",
        "        # Extract time information from available columns\n",
        "        time_slot = row.get('time_slot', 0)\n",
        "        day = row.get('day', 1)\n",
        "        \n",
        "        # Convert time_slot to hour (assuming time_slot represents hours or time periods)\n",
        "        if 'hour' in row:\n",
        "            hour = row['hour']\n",
        "        else:\n",
        "            # Estimate hour from time_slot (adjust based on your data structure)\n",
        "            hour = int(time_slot) % 24 if time_slot else 12\n",
        "            \n",
        "        # Get minute if available, otherwise use 0\n",
        "        minute = row.get('minute', 0)\n",
        "        \n",
        "        # Get day name if available\n",
        "        if 'day_name' in row:\n",
        "            day_name = row['day_name']\n",
        "        else:\n",
        "            # Generate day name from day number\n",
        "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "            day_name = days[day % 7]\n",
        "        \n",
        "        # Check weekend status\n",
        "        if 'is_weekend' in row:\n",
        "            is_weekend = row['is_weekend'] == 'True' or row['is_weekend'] == True\n",
        "        else:\n",
        "            is_weekend = (day % 7) >= 5  # Assume Saturday and Sunday are weekends\n",
        "        \n",
        "        time_str = f\"{hour:02d}:{minute:02d}\"\n",
        "        \n",
        "        if hour < 6:\n",
        "            period = \"early morning\"\n",
        "        elif hour < 12:\n",
        "            period = \"morning\"\n",
        "        elif hour < 17:\n",
        "            period = \"afternoon\"\n",
        "        elif hour < 21:\n",
        "            period = \"evening\"\n",
        "        else:\n",
        "            period = \"night\"\n",
        "        \n",
        "        weekend_context = \"weekend\" if is_weekend else \"weekday\"\n",
        "        \n",
        "        return {\n",
        "            'time_str': time_str,\n",
        "            'period': period,\n",
        "            'day_context': f\"{day_name} {weekend_context}\",\n",
        "            'full_context': f\"At {time_str} on {day_name} {period}\"\n",
        "        }\n",
        "    \n",
        "    def generate_location_context(self, row):\n",
        "        \"\"\"Generate natural language location context from available data.\"\"\"\n",
        "        # Get location information from available columns\n",
        "        location_category = row.get('location_category', 'Unknown Location')\n",
        "        location_function = row.get('location_function', 'general')\n",
        "        poi_density = row.get('poi_density', 500)\n",
        "        category_diversity = row.get('category_diversity', 10)\n",
        "        distance_from_center = row.get('distance_from_center', 50)\n",
        "        distance_quartile = row.get('distance_quartile', 'Medium')\n",
        "        \n",
        "        # Density classification\n",
        "        if poi_density > 1500:\n",
        "            density_desc = \"high-density urban area\"\n",
        "        elif poi_density > 800:\n",
        "            density_desc = \"moderate-density area\"\n",
        "        else:\n",
        "            density_desc = \"low-density area\"\n",
        "        \n",
        "        # Diversity classification\n",
        "        if category_diversity > 25:\n",
        "            diversity_desc = \"with diverse amenities\"\n",
        "        elif category_diversity > 15:\n",
        "            diversity_desc = \"with several amenities\"\n",
        "        else:\n",
        "            diversity_desc = \"with limited amenities\"\n",
        "        \n",
        "        center_context = f\"{distance_quartile.lower()} from city center\"\n",
        "        \n",
        "        return {\n",
        "            'primary_location': location_category,\n",
        "            'function': location_function,\n",
        "            'density_desc': density_desc,\n",
        "            'diversity_desc': diversity_desc,\n",
        "            'center_context': center_context,\n",
        "            'full_description': f\"{location_category} in {density_desc} {center_context} {diversity_desc}\"\n",
        "        }\n",
        "    \n",
        "    def generate_activity_context(self, row, time_period):\n",
        "        \"\"\"Generate activity context based on functional category and time.\"\"\"\n",
        "        functional_category = row.get('functional_category', None)\n",
        "        location_function = row.get('location_function', 'general')\n",
        "        \n",
        "        if functional_category and functional_category in self.activity_mapping:\n",
        "            activity = self.activity_mapping[functional_category]\n",
        "        else:\n",
        "            activity = 'general activities'\n",
        "        \n",
        "        # Time-based activity refinement\n",
        "        if time_period == 'morning' and activity == 'work':\n",
        "            activity = 'commuting to work'\n",
        "        elif time_period == 'evening' and 'residential' in str(functional_category):\n",
        "            activity = 'returning home'\n",
        "        \n",
        "        return {\n",
        "            'inferred_activity': activity,\n",
        "            'functional_category': functional_category,\n",
        "            'context': f\"engaged in {activity}\"\n",
        "        }\n",
        "    \n",
        "    def create_mobility_sentence(self, row):\n",
        "        \"\"\"Generate natural language sentence for mobility record using available columns.\"\"\"\n",
        "        \n",
        "        # Generate contexts using available data\n",
        "        time_ctx = self.generate_time_context(row)\n",
        "        location_ctx = self.generate_location_context(row)\n",
        "        activity_ctx = self.generate_activity_context(row, time_ctx['period'])\n",
        "        \n",
        "        # Get grid coordinates if available\n",
        "        grid_x = row.get('grid_x', 0)\n",
        "        grid_y = row.get('grid_y', 0)\n",
        "        day = row.get('day', 1)\n",
        "        time_slot = row.get('time_slot', 1)\n",
        "        \n",
        "        # Generate sentences for required styles\n",
        "        sentences = {\n",
        "            'medium': f\"At {time_ctx['time_str']}, user was at {location_ctx['primary_location']} in {location_ctx['density_desc']} {activity_ctx['context']}.\",\n",
        "            'detailed': f\"{time_ctx['full_context']}, the user visited {location_ctx['full_description']} and {activity_ctx['context']}.\"\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            'sentences': sentences,\n",
        "            'contexts': {\n",
        "                'time': time_ctx,\n",
        "                'location': location_ctx,\n",
        "                'activity': activity_ctx\n",
        "            },\n",
        "            'metadata': {\n",
        "                'grid_coords': (grid_x, grid_y),\n",
        "                'timestamp': f\"Day_{day}_Slot_{time_slot}\",\n",
        "                'poi_density': location_ctx.get('poi_density', 500),\n",
        "                'distance_from_center': location_ctx.get('distance_from_center', 50)\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    def create_daily_narrative(self, user_data, narrative_style='medium'):\n",
        "        \"\"\"Create daily narrative from user mobility data.\"\"\"\n",
        "        \n",
        "        # Sort by time slot if available\n",
        "        if 'time_slot' in user_data.columns:\n",
        "            user_data_sorted = user_data.sort_values('time_slot')\n",
        "        else:\n",
        "            user_data_sorted = user_data\n",
        "        \n",
        "        daily_sentences = []\n",
        "        contexts = []\n",
        "        \n",
        "        for _, row in user_data_sorted.iterrows():\n",
        "            sentence_data = self.create_mobility_sentence(row)\n",
        "            daily_sentences.append(sentence_data['sentences'][narrative_style])\n",
        "            contexts.append(sentence_data)\n",
        "        \n",
        "        # Create coherent narrative flow\n",
        "        narrative_parts = []\n",
        "        connectors = [\"Then,\", \"Next,\", \"Later,\", \"Subsequently,\"]\n",
        "        \n",
        "        for i, sentence in enumerate(daily_sentences):\n",
        "            if i == 0:\n",
        "                narrative_parts.append(f\"The day began: {sentence}\")\n",
        "            elif i == len(daily_sentences) - 1:\n",
        "                narrative_parts.append(f\"Finally, {sentence}\")\n",
        "            else:\n",
        "                connector = connectors[i % len(connectors)]\n",
        "                narrative_parts.append(f\"{connector} {sentence}\")\n",
        "        \n",
        "        return {\n",
        "            'full_narrative': ' '.join(narrative_parts),\n",
        "            'individual_sentences': daily_sentences,\n",
        "            'sentence_contexts': contexts,\n",
        "            'total_visits': len(daily_sentences)\n",
        "        }\n",
        "    \n",
        "    def process_users_in_batches(self, df, style):\n",
        "        \"\"\"Process users in memory-efficient batches.\"\"\"\n",
        "        \n",
        "        logger.info(f\"Processing {style} style for up to {self.max_users} users in batches of {self.batch_size}\")\n",
        "        \n",
        "        if 'user_id' not in df.columns:\n",
        "            logger.error(\"user_id column not found\")\n",
        "            return {}\n",
        "        \n",
        "        # Select users to process\n",
        "        all_users = df['user_id'].unique()\n",
        "        users_to_process = all_users[:self.max_users]\n",
        "        \n",
        "        logger.info(f\"Found {len(all_users)} total users, processing {len(users_to_process)}\")\n",
        "        \n",
        "        mobility_corpus = {\n",
        "            'normal_period': {},\n",
        "            'emergency_period': {}\n",
        "        }\n",
        "        \n",
        "        # Process in batches\n",
        "        for batch_start in range(0, len(users_to_process), self.batch_size):\n",
        "            batch_end = min(batch_start + self.batch_size, len(users_to_process))\n",
        "            batch_users = users_to_process[batch_start:batch_end]\n",
        "            \n",
        "            logger.info(f\"Processing batch {batch_start//self.batch_size + 1}: users {batch_start+1}-{batch_end}\")\n",
        "            \n",
        "            for user_id in batch_users:\n",
        "                user_data = df[df['user_id'] == user_id]\n",
        "                \n",
        "                # Initialize user data for both periods\n",
        "                mobility_corpus['normal_period'][f'user_{user_id}'] = {}\n",
        "                mobility_corpus['emergency_period'][f'user_{user_id}'] = {}\n",
        "                \n",
        "                # Process each day\n",
        "                for day in user_data['day'].unique():\n",
        "                    day_data = user_data[user_data['day'] == day]\n",
        "                    \n",
        "                    if len(day_data) > 0:\n",
        "                        period_type = self.get_period_type(day)\n",
        "                        \n",
        "                        if period_type in ['normal', 'emergency']:\n",
        "                            period_key = f'{period_type}_period'\n",
        "                            narrative = self.create_daily_narrative(day_data, style)\n",
        "                            mobility_corpus[period_key][f'user_{user_id}'][f'day_{day}'] = narrative\n",
        "            \n",
        "            # Memory management\n",
        "            gc.collect()\n",
        "            logger.info(f\"Batch {batch_start//self.batch_size + 1} completed\")\n",
        "        \n",
        "        return mobility_corpus\n",
        "    \n",
        "    def save_corpus_files(self, mobility_corpus, style):\n",
        "        \"\"\"Save processed corpus to files.\"\"\"\n",
        "        \n",
        "        style_path = os.path.join(self.base_path, f'{style}_style')\n",
        "        \n",
        "        # Create directories\n",
        "        for period in ['normal_period', 'emergency_period']:\n",
        "            period_path = os.path.join(style_path, period)\n",
        "            os.makedirs(period_path, exist_ok=True)\n",
        "        \n",
        "        logger.info(f\"Saving {style} style files\")\n",
        "        \n",
        "        for period_type, period_data in mobility_corpus.items():\n",
        "            period_folder = os.path.join(style_path, period_type)\n",
        "            user_count = 0\n",
        "            \n",
        "            for user_id, user_data in period_data.items():\n",
        "                if user_data:\n",
        "                    # Save JSON format\n",
        "                    user_file = os.path.join(period_folder, f'{user_id}_{period_type}.json')\n",
        "                    with open(user_file, 'w') as f:\n",
        "                        json.dump(user_data, f, indent=2)\n",
        "                    \n",
        "                    # Save readable text format\n",
        "                    text_file = os.path.join(period_folder, f'{user_id}_{period_type}_readable.txt')\n",
        "                    with open(text_file, 'w') as f:\n",
        "                        f.write(f\"User: {user_id} - Period: {period_type} - Style: {style}\\n\")\n",
        "                        f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "                        \n",
        "                        for day_id, day_data in user_data.items():\n",
        "                            f.write(f\"Day: {day_id}\\n\")\n",
        "                            f.write(\"-\" * 40 + \"\\n\")\n",
        "                            f.write(f\"Narrative: {day_data['full_narrative']}\\n\\n\")\n",
        "                            f.write(\"Individual Sentences:\\n\")\n",
        "                            for i, sentence in enumerate(day_data['individual_sentences'], 1):\n",
        "                                f.write(f\"{i}. {sentence}\\n\")\n",
        "                            f.write(f\"\\nTotal visits: {day_data['total_visits']}\\n\")\n",
        "                            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "                    \n",
        "                    user_count += 1\n",
        "            \n",
        "            logger.info(f\"Saved {user_count} users for {period_type}\")\n",
        "    \n",
        "    def export_training_data(self, mobility_corpus, style):\n",
        "        \"\"\"Export data in formats suitable for machine learning training.\"\"\"\n",
        "        \n",
        "        style_path = os.path.join(self.base_path, f'{style}_style')\n",
        "        \n",
        "        for period_type, period_data in mobility_corpus.items():\n",
        "            \n",
        "            # JSONL format for language model training\n",
        "            jsonl_file = os.path.join(style_path, period_type, f'training_data_{period_type}_{style}.jsonl')\n",
        "            \n",
        "            with open(jsonl_file, 'w') as f:\n",
        "                for user_id, user_data in period_data.items():\n",
        "                    for day_id, day_data in user_data.items():\n",
        "                        sample = {\n",
        "                            'user_id': user_id,\n",
        "                            'day': day_id,\n",
        "                            'period_type': period_type,\n",
        "                            'style': style,\n",
        "                            'text': day_data['full_narrative'],\n",
        "                            'sentences': day_data['individual_sentences'],\n",
        "                            'metadata': {\n",
        "                                'total_visits': day_data['total_visits'],\n",
        "                                'period': period_type,\n",
        "                                'style': style\n",
        "                            }\n",
        "                        }\n",
        "                        f.write(json.dumps(sample) + '\\n')\n",
        "            \n",
        "            # Plain text format\n",
        "            txt_file = os.path.join(style_path, period_type, f'gpt_training_{period_type}_{style}.txt')\n",
        "            \n",
        "            with open(txt_file, 'w') as f:\n",
        "                f.write(f\"Mobility Text Corpus - {period_type.title()} Period - {style.title()} Style\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "                \n",
        "                for user_data in period_data.values():\n",
        "                    for day_data in user_data.values():\n",
        "                        f.write(day_data['full_narrative'] + '\\n\\n')\n",
        "    \n",
        "    def generate_processing_statistics(self, mobility_df):\n",
        "        \"\"\"Generate comprehensive processing statistics.\"\"\"\n",
        "        \n",
        "        stats_file = os.path.join(self.base_path, 'continuation_processing_statistics.txt')\n",
        "        \n",
        "        with open(stats_file, 'w') as f:\n",
        "            f.write(\"Mobility Text Generation - Continuation Processing Statistics\\n\")\n",
        "            f.write(\"IEEE Transactions on Knowledge and Data Engineering (TKDE)\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "            \n",
        "            f.write(\"Processing Configuration:\\n\")\n",
        "            f.write(f\"  Narrative Styles: Medium, Detailed\\n\")\n",
        "            f.write(f\"  Maximum Users per Style: {self.max_users}\\n\")\n",
        "            f.write(f\"  Batch Size: {self.batch_size} users\\n\")\n",
        "            f.write(f\"  Memory Optimization: Enabled\\n\\n\")\n",
        "            \n",
        "            f.write(\"Dataset Statistics:\\n\")\n",
        "            f.write(f\"  Total Records: {len(mobility_df):,}\\n\")\n",
        "            f.write(f\"  Dataset Columns: {list(mobility_df.columns)}\\n\\n\")\n",
        "            \n",
        "            if 'user_id' in mobility_df.columns:\n",
        "                f.write(f\"  Total Users Available: {mobility_df['user_id'].nunique():,}\\n\")\n",
        "                f.write(f\"  Users Processed: {min(self.max_users, mobility_df['user_id'].nunique()):,}\\n\")\n",
        "            \n",
        "            if 'day' in mobility_df.columns:\n",
        "                total_days = mobility_df['day'].nunique()\n",
        "                normal_days = mobility_df[mobility_df['day'] <= 60]['day'].nunique()\n",
        "                emergency_days = mobility_df[(mobility_df['day'] > 60) & (mobility_df['day'] <= 75)]['day'].nunique()\n",
        "                \n",
        "                f.write(f\"  Total Days: {total_days}\\n\")\n",
        "                f.write(f\"  Normal Period Days (1-60): {normal_days}\\n\")\n",
        "                f.write(f\"  Emergency Period Days (61-75): {emergency_days}\\n\\n\")\n",
        "                \n",
        "                f.write(\"Period Analysis:\\n\")\n",
        "                normal_records = len(mobility_df[mobility_df['day'] <= 60])\n",
        "                emergency_records = len(mobility_df[(mobility_df['day'] > 60) & (mobility_df['day'] <= 75)])\n",
        "                \n",
        "                f.write(f\"  Normal Period Records: {normal_records:,}\\n\")\n",
        "                f.write(f\"  Emergency Period Records: {emergency_records:,}\\n\")\n",
        "                \n",
        "                if normal_records > 0 and emergency_records > 0:\n",
        "                    f.write(f\"  Record Distribution Ratio: {normal_records/emergency_records:.2f}:1\\n\")\n",
        "        \n",
        "        logger.info(f\"Statistics saved to {stats_file}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function for continuation processing.\"\"\"\n",
        "    \n",
        "    logger.info(\"Starting mobility text generation continuation processing\")\n",
        "    logger.info(\"Target: IEEE Transactions on Knowledge and Data Engineering (TKDE)\")\n",
        "    \n",
        "    # Initialize processor\n",
        "    processor = MobilityTextProcessor(max_users=1000, batch_size=50)\n",
        "    \n",
        "    # Create base directory\n",
        "    os.makedirs(processor.base_path, exist_ok=True)\n",
        "    \n",
        "    try:\n",
        "        # Load datasets\n",
        "        mobility_df = processor.load_mobility_datasets()\n",
        "        \n",
        "        if mobility_df is None:\n",
        "            logger.error(\"Failed to load mobility datasets\")\n",
        "            return\n",
        "        \n",
        "        logger.info(f\"Processing {len(mobility_df):,} mobility records\")\n",
        "        \n",
        "        # Process medium style\n",
        "        logger.info(\"Processing medium narrative style\")\n",
        "        medium_corpus = processor.process_users_in_batches(mobility_df, 'medium')\n",
        "        processor.save_corpus_files(medium_corpus, 'medium')\n",
        "        processor.export_training_data(medium_corpus, 'medium')\n",
        "        \n",
        "        # Clear memory\n",
        "        del medium_corpus\n",
        "        gc.collect()\n",
        "        logger.info(\"Medium style processing completed\")\n",
        "        \n",
        "        # Process detailed style\n",
        "        logger.info(\"Processing detailed narrative style\")\n",
        "        detailed_corpus = processor.process_users_in_batches(mobility_df, 'detailed')\n",
        "        processor.save_corpus_files(detailed_corpus, 'detailed')\n",
        "        processor.export_training_data(detailed_corpus, 'detailed')\n",
        "        \n",
        "        # Clear memory\n",
        "        del detailed_corpus\n",
        "        gc.collect()\n",
        "        logger.info(\"Detailed style processing completed\")\n",
        "        \n",
        "        # Generate statistics\n",
        "        processor.generate_processing_statistics(mobility_df)\n",
        "        \n",
        "        logger.info(\"Continuation processing completed successfully\")\n",
        "        logger.info(f\"Output files saved in: {processor.base_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main processing: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
