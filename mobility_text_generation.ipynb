{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1754451214459
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace existing load functions with these Azure-compatible versions\n",
        "\n",
        "def load_mobility_datasets():\n",
        "    \"\"\"Load all mobility datasets and merge them - Azure compatible\"\"\"\n",
        "    try:\n",
        "        # Try relative paths first (for uploaded files)\n",
        "        base_df = pd.read_csv(\"scientific_mobility_base_dataset.csv\")\n",
        "        functional_df = pd.read_csv(\"scientific_mobility_functional_dataset.csv\")\n",
        "    except FileNotFoundError:\n",
        "        # If files not found, provide helpful error message\n",
        "        print(\"ERROR: CSV files not found!\")\n",
        "        print(\"Please ensure these files are uploaded to your Azure workspace:\")\n",
        "        print(\"- scientific_mobility_base_dataset.csv\")\n",
        "        print(\"- scientific_mobility_functional_dataset.csv\")\n",
        "        print(\"- scientific_mobility_poi_matrix_dataset.csv\")\n",
        "        return None\n",
        "    \n",
        "    # Merge base and functional datasets\n",
        "    merged_df = base_df.merge(\n",
        "        functional_df[['user_id', 'day', 'time_slot', 'functional_category']], \n",
        "        on=['user_id', 'day', 'time_slot'], \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    return merged_df"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1754451214637
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_poi_presence_matrix():\n",
        "    \"\"\"Load and process the POI presence matrix - Azure compatible\"\"\"\n",
        "    try:\n",
        "        poi_df = pd.read_csv(\"scientific_mobility_poi_matrix_dataset.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"WARNING: POI matrix file not found. Continuing without POI features.\")\n",
        "        return None, [], []\n",
        "    \n",
        "    # Get POI binary columns (assuming they start with 'has_')\n",
        "    poi_columns = [col for col in poi_df.columns if col.startswith('has_')]\n",
        "    \n",
        "    # Get unique POI categories and functions\n",
        "    poi_categories = [col.replace('has_', '') for col in poi_columns \n",
        "                     if 'category' in col.lower() or len(col.split('_')) <= 3]\n",
        "    \n",
        "    return poi_df, poi_columns, poi_categories"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1754451214806
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_period_type(day):\n",
        "    \"\"\"Determine if day is in normal or emergency period\"\"\"\n",
        "    if day <= 60:\n",
        "        return 'normal'\n",
        "    elif day <= 75:\n",
        "        return 'emergency'\n",
        "    else:\n",
        "        return 'post_emergency'\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1754451214979
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_user_folders(base_path='UserMobilityTexts'):\n",
        "    \"\"\"Create folder structure for user mobility texts - Azure compatible\"\"\"\n",
        "    if not os.path.exists(base_path):\n",
        "        os.makedirs(base_path)\n",
        "    \n",
        "    subfolders = ['normal_period', 'emergency_period', 'daily_narratives', 'sequences', 'vocabulary']\n",
        "    for subfolder in subfolders:\n",
        "        folder_path = os.path.join(base_path, subfolder)\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "    \n",
        "    return base_path"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1754451215132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create time-based context descriptions\n",
        "def generate_time_context(row):\n",
        "    \"\"\"Generate natural language time context with missing column handling\"\"\"\n",
        "    \n",
        "    # Handle hour - try different possible column names\n",
        "    if 'hour' in row:\n",
        "        hour = row['hour']\n",
        "    elif 'time_slot' in row:\n",
        "        # If only time_slot exists, convert it to hour (assuming time_slot 0-23 = hours)\n",
        "        hour = row['time_slot'] % 24\n",
        "    else:\n",
        "        hour = 12  # Default fallback\n",
        "    \n",
        "    # Handle minute - create default if missing\n",
        "    if 'minute' in row:\n",
        "        minute = row['minute']\n",
        "    else:\n",
        "        # Generate minute from time_slot if available, otherwise default\n",
        "        if 'time_slot' in row:\n",
        "            minute = (row['time_slot'] * 15) % 60  # Assume 15-min intervals\n",
        "        else:\n",
        "            minute = 0  # Default fallback\n",
        "    \n",
        "    # Handle day_name\n",
        "    if 'day_name' in row:\n",
        "        day_name = row['day_name']\n",
        "    else:\n",
        "        # Generate from day number if available\n",
        "        if 'day' in row:\n",
        "            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "            day_name = days[row['day'] % 7]\n",
        "        else:\n",
        "            day_name = \"Monday\"  # Default fallback\n",
        "    \n",
        "    # Handle weekend flag\n",
        "    if 'is_weekend' in row:\n",
        "        is_weekend = row['is_weekend'] == 'True' or row['is_weekend'] == True\n",
        "    else:\n",
        "        # Infer from day if available\n",
        "        if 'day' in row:\n",
        "            is_weekend = (row['day'] % 7) >= 5  # Saturday=5, Sunday=6\n",
        "        else:\n",
        "            is_weekend = False  # Default fallback\n",
        "    \n",
        "    time_str = f\"{hour:02d}:{minute:02d}\"\n",
        "    \n",
        "    if hour < 6:\n",
        "        period = \"early morning\"\n",
        "    elif hour < 12:\n",
        "        period = \"morning\"\n",
        "    elif hour < 17:\n",
        "        period = \"afternoon\"\n",
        "    elif hour < 21:\n",
        "        period = \"evening\"\n",
        "    else:\n",
        "        period = \"night\"\n",
        "    \n",
        "    weekend_context = \"weekend\" if is_weekend else \"weekday\"\n",
        "    \n",
        "    return {\n",
        "        'time_str': time_str,\n",
        "        'period': period,\n",
        "        'day_context': f\"{day_name} {weekend_context}\",\n",
        "        'full_context': f\"At {time_str} on {day_name} {period}\"\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1754451215274
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Create location context from POI density and categories\n",
        "def generate_location_context(row):\n",
        "    \"\"\"Generate natural language location context with missing column handling\"\"\"\n",
        "    \n",
        "    # Handle location category\n",
        "    location_category = row.get('location_category', 'unknown location')\n",
        "    location_function = row.get('location_function', 'general purpose')\n",
        "    \n",
        "    # Handle POI density\n",
        "    poi_density = row.get('poi_density', 1000)  # Default moderate density\n",
        "    \n",
        "    # Handle category diversity\n",
        "    category_diversity = row.get('category_diversity', 15)  # Default moderate diversity\n",
        "    \n",
        "    # Handle distance from center\n",
        "    distance_from_center = row.get('distance_from_center', 5.0)  # Default 5km\n",
        "    distance_quartile = row.get('distance_quartile', 'moderate')  # Default quartile\n",
        "    \n",
        "    # Density description\n",
        "    if poi_density > 1500:\n",
        "        density_desc = \"high-density urban area\"\n",
        "    elif poi_density > 800:\n",
        "        density_desc = \"moderate-density area\"\n",
        "    else:\n",
        "        density_desc = \"low-density area\"\n",
        "    \n",
        "    # Diversity description\n",
        "    if category_diversity > 25:\n",
        "        diversity_desc = \"with diverse amenities\"\n",
        "    elif category_diversity > 15:\n",
        "        diversity_desc = \"with several amenities\"\n",
        "    else:\n",
        "        diversity_desc = \"with limited amenities\"\n",
        "    \n",
        "    # Distance context\n",
        "    if isinstance(distance_quartile, str):\n",
        "        center_context = f\"{distance_quartile.lower()} from city center\"\n",
        "    else:\n",
        "        center_context = \"moderate distance from city center\"\n",
        "    \n",
        "    return {\n",
        "        'primary_location': location_category,\n",
        "        'function': location_function,\n",
        "        'density_desc': density_desc,\n",
        "        'diversity_desc': diversity_desc,\n",
        "        'center_context': center_context,\n",
        "        'full_description': f\"{location_category} in {density_desc} {center_context} {diversity_desc}\"\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1754451215430
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate activity context based on functional category\n",
        "def generate_activity_context(functional_category, location_function, time_period):\n",
        "    \"\"\"Generate activity-based context\"\"\"\n",
        "    \n",
        "    activity_mapping = {\n",
        "        'food_dining': ['dining', 'eating', 'food shopping'],\n",
        "        'retail_shopping': ['shopping', 'browsing stores', 'purchasing items'],\n",
        "        'entertainment': ['entertainment', 'leisure activities', 'recreation'],\n",
        "        'professional': ['work', 'business activities', 'professional meetings'],\n",
        "        'healthcare': ['medical appointment', 'health services', 'wellness'],\n",
        "        'education': ['learning', 'educational activities', 'studying'],\n",
        "        'transportation': ['transit', 'traveling', 'commuting'],\n",
        "        'residential': ['home activities', 'residential area', 'living']\n",
        "    }\n",
        "    \n",
        "    # Infer likely activity\n",
        "    if functional_category and functional_category in activity_mapping:\n",
        "        activities = activity_mapping[functional_category]\n",
        "    else:\n",
        "        activities = ['general activities']\n",
        "    \n",
        "    # Time-based activity refinement\n",
        "    if time_period == 'morning' and 'work' in activities:\n",
        "        activity = 'commuting to work'\n",
        "    elif time_period == 'evening' and 'residential' in activities:\n",
        "        activity = 'returning home'\n",
        "    else:\n",
        "        activity = activities[0] if activities else 'visiting location'\n",
        "    \n",
        "    return {\n",
        "        'inferred_activity': activity,\n",
        "        'functional_category': functional_category,\n",
        "        'context': f\"engaged in {activity}\"\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1754451215568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mobility_sentence(row):\n",
        "    \"\"\"FIXED: Create a natural language sentence for a single mobility record\"\"\"\n",
        "    \n",
        "    # Generate all contexts using safe functions\n",
        "    time_ctx = generate_time_context(row)\n",
        "    location_ctx = generate_location_context(row)\n",
        "    \n",
        "    activity_ctx = generate_activity_context(\n",
        "        row.get('functional_category', None),\n",
        "        row.get('location_function', 'general purpose'),\n",
        "        time_ctx['period']\n",
        "    )\n",
        "    \n",
        "    # Handle grid coordinates\n",
        "    grid_x = row.get('grid_x', 0)\n",
        "    grid_y = row.get('grid_y', 0)\n",
        "    \n",
        "    # Construct sentence variations\n",
        "    sentences = {\n",
        "        'detailed': f\"{time_ctx['full_context']}, the user visited {location_ctx['full_description']} and {activity_ctx['context']}.\",\n",
        "        \n",
        "        'medium': f\"At {time_ctx['time_str']}, user was at {row.get('location_category', 'location')} in {location_ctx['density_desc']} {activity_ctx['context']}.\",\n",
        "        \n",
        "        'simple': f\"{time_ctx['period']}: {row.get('location_category', 'location')} - {activity_ctx['inferred_activity']}\",\n",
        "        \n",
        "        'coordinate': f\"Grid({grid_x}, {grid_y}) at {time_ctx['time_str']}: {row.get('location_category', 'location')}\"\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'sentences': sentences,\n",
        "        'contexts': {\n",
        "            'time': time_ctx,\n",
        "            'location': location_ctx,\n",
        "            'activity': activity_ctx\n",
        "        },\n",
        "        'metadata': {\n",
        "            'grid_coords': (grid_x, grid_y),\n",
        "            'timestamp': f\"Day_{row.get('day', 0)}_Slot_{row.get('time_slot', 0)}\",\n",
        "            'poi_density': row.get('poi_density', 1000),\n",
        "            'distance_from_center': row.get('distance_from_center', 5.0)\n",
        "        }\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1754451215706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Generate daily mobility narratives\n",
        "def create_daily_narrative(user_data, narrative_style='medium'):\n",
        "    \"\"\"FIXED: Create a daily narrative for a user's mobility\"\"\"\n",
        "    \n",
        "    # Sort by time_slot\n",
        "    user_data_sorted = user_data.sort_values('time_slot')\n",
        "    \n",
        "    daily_sentences = []\n",
        "    contexts = []\n",
        "    \n",
        "    for _, row in user_data_sorted.iterrows():\n",
        "        sentence_data = create_mobility_sentence(row)  # Use fixed function\n",
        "        daily_sentences.append(sentence_data['sentences'][narrative_style])\n",
        "        contexts.append(sentence_data)\n",
        "    \n",
        "    # Create narrative flow\n",
        "    narrative_parts = []\n",
        "    for i, sentence in enumerate(daily_sentences):\n",
        "        if i == 0:\n",
        "            narrative_parts.append(f\"The day began: {sentence}\")\n",
        "        elif i == len(daily_sentences) - 1:\n",
        "            narrative_parts.append(f\"Finally, {sentence}\")\n",
        "        else:\n",
        "            connectors = [\"Then,\", \"Next,\", \"Later,\", \"Subsequently,\"]\n",
        "            connector = connectors[i % len(connectors)]\n",
        "            narrative_parts.append(f\"{connector} {sentence}\")\n",
        "    \n",
        "    return {\n",
        "        'full_narrative': ' '.join(narrative_parts),\n",
        "        'individual_sentences': daily_sentences,\n",
        "        'sentence_contexts': contexts,\n",
        "        'total_visits': len(daily_sentences)\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1754451215846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Generate mobility corpus separated by periods\n",
        "def generate_mobility_corpus_by_period(df, output_style='medium'):\n",
        "    \"\"\"Generate mobility text corpus separated by normal/emergency periods\"\"\"\n",
        "    \n",
        "    mobility_corpus = {\n",
        "        'normal_period': {},\n",
        "        'emergency_period': {}\n",
        "    }\n",
        "    \n",
        "    for user_id in df['user_id'].unique():\n",
        "        user_data = df[df['user_id'] == user_id]\n",
        "        \n",
        "        # Initialize user data for both periods\n",
        "        mobility_corpus['normal_period'][f'user_{user_id}'] = {}\n",
        "        mobility_corpus['emergency_period'][f'user_{user_id}'] = {}\n",
        "        \n",
        "        for day in user_data['day'].unique():\n",
        "            day_data = user_data[user_data['day'] == day]\n",
        "            \n",
        "            if len(day_data) > 0:\n",
        "                # Determine period type\n",
        "                period_type = get_period_type(day)\n",
        "                \n",
        "                if period_type in ['normal', 'emergency']:\n",
        "                    period_key = f'{period_type}_period'\n",
        "                    narrative = create_daily_narrative(day_data, output_style)\n",
        "                    mobility_corpus[period_key][f'user_{user_id}'][f'day_{day}'] = narrative\n",
        "    \n",
        "    return mobility_corpus"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1754451215989
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW FUNCTION: Save individual user files\n",
        "def save_user_files_by_period(mobility_corpus, base_path):\n",
        "    \"\"\"Save individual files for each user separated by period\"\"\"\n",
        "    \n",
        "    for period_type, period_data in mobility_corpus.items():\n",
        "        period_folder = os.path.join(base_path, period_type)\n",
        "        \n",
        "        for user_id, user_data in period_data.items():\n",
        "            if user_data:  # Only save if user has data for this period\n",
        "                \n",
        "                # Create user-specific files\n",
        "                user_file = os.path.join(period_folder, f'{user_id}_{period_type}.json')\n",
        "                \n",
        "                # Save detailed JSON with all contexts\n",
        "                with open(user_file, 'w') as f:\n",
        "                    json.dump(user_data, f, indent=2)\n",
        "                \n",
        "                # Create readable text file for each user\n",
        "                text_file = os.path.join(period_folder, f'{user_id}_{period_type}_readable.txt')\n",
        "                with open(text_file, 'w') as f:\n",
        "                    f.write(f\"=== {user_id.upper()} - {period_type.upper()} ===\\n\\n\")\n",
        "                    \n",
        "                    for day_id, day_data in user_data.items():\n",
        "                        f.write(f\"\\n--- {day_id.upper()} ---\\n\")\n",
        "                        f.write(f\"Full Narrative:\\n{day_data['full_narrative']}\\n\\n\")\n",
        "                        f.write(f\"Individual Sentences:\\n\")\n",
        "                        for i, sentence in enumerate(day_data['individual_sentences'], 1):\n",
        "                            f.write(f\"{i}. {sentence}\\n\")\n",
        "                        f.write(f\"\\nTotal visits: {day_data['total_visits']}\\n\")\n",
        "                        f.write(\"-\" * 80 + \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1754451216128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW FUNCTION: Export training data by period\n",
        "def export_training_data_by_period(mobility_corpus, base_path):\n",
        "    \"\"\"Export training data separated by period\"\"\"\n",
        "    \n",
        "    for period_type, period_data in mobility_corpus.items():\n",
        "        \n",
        "        # JSONL format for language model training\n",
        "        jsonl_file = os.path.join(base_path, period_type, f'training_data_{period_type}.jsonl')\n",
        "        \n",
        "        with open(jsonl_file, 'w') as f:\n",
        "            for user_id, user_data in period_data.items():\n",
        "                for day_id, day_data in user_data.items():\n",
        "                    sample = {\n",
        "                        'user_id': user_id,\n",
        "                        'day': day_id,\n",
        "                        'period_type': period_type,\n",
        "                        'text': day_data['full_narrative'],\n",
        "                        'sentences': day_data['individual_sentences'],\n",
        "                        'metadata': {\n",
        "                            'total_visits': day_data['total_visits'],\n",
        "                            'period': period_type\n",
        "                        }\n",
        "                    }\n",
        "                    f.write(json.dumps(sample) + '\\n')\n",
        "        \n",
        "        # Plain text format for GPT training\n",
        "        txt_file = os.path.join(base_path, period_type, f'gpt_training_{period_type}.txt')\n",
        "        \n",
        "        with open(txt_file, 'w') as f:\n",
        "            f.write(f\"=== MOBILITY TEXTS - {period_type.upper()} ===\\n\\n\")\n",
        "            \n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    f.write(day_data['full_narrative'] + '\\n')\n",
        "                    f.write('\\n')"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1754451216265
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW FUNCTION: Create user summaries\n",
        "def create_user_summary_files(mobility_corpus, base_path):\n",
        "    \"\"\"Create summary files for each user across both periods\"\"\"\n",
        "    \n",
        "    summary_folder = os.path.join(base_path, 'user_summaries')\n",
        "    if not os.path.exists(summary_folder):\n",
        "        os.makedirs(summary_folder)\n",
        "    \n",
        "    # Get all unique users across both periods\n",
        "    all_users = set()\n",
        "    for period_data in mobility_corpus.values():\n",
        "        all_users.update(period_data.keys())\n",
        "    \n",
        "    for user_id in all_users:\n",
        "        summary_file = os.path.join(summary_folder, f'{user_id}_complete_summary.txt')\n",
        "        \n",
        "        with open(summary_file, 'w') as f:\n",
        "            f.write(f\"=== COMPLETE MOBILITY SUMMARY FOR {user_id.upper()} ===\\n\\n\")\n",
        "            \n",
        "            for period_type in ['normal_period', 'emergency_period']:\n",
        "                f.write(f\"\\n{'='*50}\\n\")\n",
        "                f.write(f\"{period_type.upper().replace('_', ' ')}\\n\")\n",
        "                f.write(f\"{'='*50}\\n\")\n",
        "                \n",
        "                if user_id in mobility_corpus[period_type]:\n",
        "                    user_data = mobility_corpus[period_type][user_id]\n",
        "                    \n",
        "                    f.write(f\"Total days: {len(user_data)}\\n\")\n",
        "                    f.write(f\"Days covered: {list(user_data.keys())}\\n\\n\")\n",
        "                    \n",
        "                    total_visits = sum(day_data['total_visits'] for day_data in user_data.values())\n",
        "                    f.write(f\"Total visits in period: {total_visits}\\n\\n\")\n",
        "                    \n",
        "                    # Sample narratives\n",
        "                    for day_id, day_data in list(user_data.items())[:3]:  # First 3 days as sample\n",
        "                        f.write(f\"\\nSample - {day_id}:\\n\")\n",
        "                        f.write(f\"{day_data['full_narrative']}\\n\")\n",
        "                        f.write(\"-\" * 40 + \"\\n\")\n",
        "                    \n",
        "                    if len(user_data) > 3:\n",
        "                        f.write(f\"\\n... and {len(user_data) - 3} more days\\n\")\n",
        "                else:\n",
        "                    f.write(\"No data available for this period\\n\")\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1754451216414
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Build vocabulary from period-separated mobility text corpus\n",
        "def build_mobility_vocabulary_updated(mobility_corpus):\n",
        "    \"\"\"Build vocabulary from period-separated mobility text corpus\"\"\"\n",
        "    \n",
        "    vocabulary = {\n",
        "        'locations': set(),\n",
        "        'activities': set(),\n",
        "        'time_periods': set(),\n",
        "        'density_terms': set(),\n",
        "        'spatial_terms': set()\n",
        "    }\n",
        "    \n",
        "    # Handle new period-separated structure\n",
        "    if 'normal_period' in mobility_corpus and 'emergency_period' in mobility_corpus:\n",
        "        # New structure: corpus has 'normal_period' and 'emergency_period' keys\n",
        "        for period_data in mobility_corpus.values():\n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    if 'sentence_contexts' in day_data:\n",
        "                        for context in day_data['sentence_contexts']:\n",
        "                            vocab_ctx = context['contexts']\n",
        "                            \n",
        "                            vocabulary['locations'].add(vocab_ctx['location']['primary_location'])\n",
        "                            vocabulary['activities'].add(vocab_ctx['activity']['inferred_activity'])\n",
        "                            vocabulary['time_periods'].add(vocab_ctx['time']['period'])\n",
        "                            vocabulary['density_terms'].add(vocab_ctx['location']['density_desc'])\n",
        "    else:\n",
        "        # Old structure: corpus directly contains user data\n",
        "        for user_data in mobility_corpus.values():\n",
        "            for day_data in user_data.values():\n",
        "                if 'sentence_contexts' in day_data:\n",
        "                    for context in day_data['sentence_contexts']:\n",
        "                        vocab_ctx = context['contexts']\n",
        "                        \n",
        "                        vocabulary['locations'].add(vocab_ctx['location']['primary_location'])\n",
        "                        vocabulary['activities'].add(vocab_ctx['activity']['inferred_activity'])\n",
        "                        vocabulary['time_periods'].add(vocab_ctx['time']['period'])\n",
        "                        vocabulary['density_terms'].add(vocab_ctx['location']['density_desc'])\n",
        "    \n",
        "    # Convert sets to sorted lists\n",
        "    for key in vocabulary:\n",
        "        vocabulary[key] = sorted(list(vocabulary[key]))\n",
        "    \n",
        "    return vocabulary"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1754451216564
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 10: Export functions for different use cases\n",
        "def export_for_language_model_training(mobility_corpus, output_file='Users/agugire/mobility_training_data.jsonl'):\n",
        "    \"\"\"Export in format suitable for language model training\"\"\"\n",
        "    \n",
        "    training_data = []\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in mobility_corpus and 'emergency_period' in mobility_corpus:\n",
        "        for period_type, period_data in mobility_corpus.items():\n",
        "            for user_id, user_data in period_data.items():\n",
        "                for day_id, day_data in user_data.items():\n",
        "                    \n",
        "                    # Create training sample\n",
        "                    sample = {\n",
        "                        'user_id': user_id,\n",
        "                        'day': day_id,\n",
        "                        'period': period_type,\n",
        "                        'text': day_data['full_narrative'],\n",
        "                        'sentences': day_data['individual_sentences'],\n",
        "                        'metadata': {\n",
        "                            'total_visits': day_data['total_visits'],\n",
        "                            'coordinates': [ctx['metadata']['grid_coords'] \n",
        "                                          for ctx in day_data['sentence_contexts']]\n",
        "                        }\n",
        "                    }\n",
        "                    \n",
        "                    training_data.append(sample)\n",
        "    else:\n",
        "        # Old structure\n",
        "        for user_id, user_data in mobility_corpus.items():\n",
        "            for day_id, day_data in user_data.items():\n",
        "                \n",
        "                # Create training sample\n",
        "                sample = {\n",
        "                    'user_id': user_id,\n",
        "                    'day': day_id,\n",
        "                    'text': day_data['full_narrative'],\n",
        "                    'sentences': day_data['individual_sentences'],\n",
        "                    'metadata': {\n",
        "                        'total_visits': day_data['total_visits'],\n",
        "                        'coordinates': [ctx['metadata']['grid_coords'] \n",
        "                                      for ctx in day_data['sentence_contexts']]\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                training_data.append(sample)\n",
        "    \n",
        "    # Save as JSONL for easy loading\n",
        "    with open(output_file, 'w') as f:\n",
        "        for sample in training_data:\n",
        "            f.write(json.dumps(sample) + '\\n')\n",
        "    \n",
        "    return training_data"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1754451216714
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_for_sequence_modeling(mobility_corpus, sequence_length=10):\n",
        "    \"\"\"Export in sequences suitable for transformer training\"\"\"\n",
        "    \n",
        "    sequences = []\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in mobility_corpus and 'emergency_period' in mobility_corpus:\n",
        "        for period_data in mobility_corpus.values():\n",
        "            for user_data in period_data.values():\n",
        "                user_sentences = []\n",
        "                \n",
        "                # Collect all sentences for user\n",
        "                for day_data in user_data.values():\n",
        "                    user_sentences.extend(day_data['individual_sentences'])\n",
        "                \n",
        "                # Create overlapping sequences\n",
        "                for i in range(0, len(user_sentences) - sequence_length + 1, sequence_length//2):\n",
        "                    sequence = user_sentences[i:i + sequence_length]\n",
        "                    if len(sequence) == sequence_length:\n",
        "                        sequences.append({\n",
        "                            'input_sequence': sequence[:-1],\n",
        "                            'target_sequence': sequence[1:],\n",
        "                            'full_sequence': sequence\n",
        "                        })\n",
        "    else:\n",
        "        # Old structure\n",
        "        for user_data in mobility_corpus.values():\n",
        "            user_sentences = []\n",
        "            \n",
        "            # Collect all sentences for user\n",
        "            for day_data in user_data.values():\n",
        "                user_sentences.extend(day_data['individual_sentences'])\n",
        "            \n",
        "            # Create overlapping sequences\n",
        "            for i in range(0, len(user_sentences) - sequence_length + 1, sequence_length//2):\n",
        "                sequence = user_sentences[i:i + sequence_length]\n",
        "                if len(sequence) == sequence_length:\n",
        "                    sequences.append({\n",
        "                        'input_sequence': sequence[:-1],\n",
        "                        'target_sequence': sequence[1:],\n",
        "                        'full_sequence': sequence\n",
        "                    })\n",
        "    \n",
        "    return sequences"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1754451216865
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Extract mobility patterns from period-separated corpus\n",
        "def extract_mobility_patterns(corpus):\n",
        "    \"\"\"Extract common mobility patterns from the period-separated text corpus\"\"\"\n",
        "    \n",
        "    patterns = {\n",
        "        'morning_activities': [],\n",
        "        'evening_activities': [],\n",
        "        'weekend_patterns': [],\n",
        "        'weekday_patterns': [],\n",
        "        'normal_period_patterns': [],\n",
        "        'emergency_period_patterns': []\n",
        "    }\n",
        "    \n",
        "    # Handle new period-separated structure\n",
        "    if 'normal_period' in corpus and 'emergency_period' in corpus:\n",
        "        # New structure: corpus has 'normal_period' and 'emergency_period' keys\n",
        "        for period_type, period_data in corpus.items():\n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    if 'sentence_contexts' in day_data:\n",
        "                        for context in day_data['sentence_contexts']:\n",
        "                            time_ctx = context['contexts']['time']\n",
        "                            activity = context['contexts']['activity']['inferred_activity']\n",
        "                            \n",
        "                            # Time-based patterns\n",
        "                            if time_ctx['period'] == 'morning':\n",
        "                                patterns['morning_activities'].append(activity)\n",
        "                            elif time_ctx['period'] == 'evening':\n",
        "                                patterns['evening_activities'].append(activity)\n",
        "                            \n",
        "                            # Weekend/weekday patterns\n",
        "                            if 'weekend' in time_ctx['day_context']:\n",
        "                                patterns['weekend_patterns'].append(activity)\n",
        "                            else:\n",
        "                                patterns['weekday_patterns'].append(activity)\n",
        "                            \n",
        "                            # Period-specific patterns\n",
        "                            if period_type == 'normal_period':\n",
        "                                patterns['normal_period_patterns'].append(activity)\n",
        "                            elif period_type == 'emergency_period':\n",
        "                                patterns['emergency_period_patterns'].append(activity)\n",
        "    else:\n",
        "        # Old structure: corpus directly contains user data\n",
        "        for user_data in corpus.values():\n",
        "            for day_data in user_data.values():\n",
        "                if 'sentence_contexts' in day_data:\n",
        "                    for context in day_data['sentence_contexts']:\n",
        "                        time_ctx = context['contexts']['time']\n",
        "                        activity = context['contexts']['activity']['inferred_activity']\n",
        "                        \n",
        "                        if time_ctx['period'] == 'morning':\n",
        "                            patterns['morning_activities'].append(activity)\n",
        "                        elif time_ctx['period'] == 'evening':\n",
        "                            patterns['evening_activities'].append(activity)\n",
        "                        \n",
        "                        if 'weekend' in time_ctx['day_context']:\n",
        "                            patterns['weekend_patterns'].append(activity)\n",
        "                        else:\n",
        "                            patterns['weekday_patterns'].append(activity)\n",
        "    \n",
        "    # Count frequencies\n",
        "    from collections import Counter\n",
        "    for pattern_type, activities in patterns.items():\n",
        "        if activities:  # Only process if we have data\n",
        "            patterns[pattern_type] = Counter(activities).most_common(10)\n",
        "        else:\n",
        "            patterns[pattern_type] = []\n",
        "    \n",
        "    return patterns"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1754451217022
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Analyze user narratives for period-separated data\n",
        "def analyze_user_narratives_updated(corpus, user_id='user_0'):\n",
        "    \"\"\"Analyze narratives for a specific user in period-separated structure\"\"\"\n",
        "    \n",
        "    print(f\"\\nAnalysis for {user_id}:\")\n",
        "    \n",
        "    # Handle new period-separated structure\n",
        "    if 'normal_period' in corpus and 'emergency_period' in corpus:\n",
        "        for period_type in ['normal_period', 'emergency_period']:\n",
        "            print(f\"\\n--- {period_type.upper().replace('_', ' ')} ---\")\n",
        "            \n",
        "            if user_id in corpus[period_type]:\n",
        "                user_data = corpus[period_type][user_id]\n",
        "                print(f\"Total days: {len(user_data)}\")\n",
        "                \n",
        "                for day_id, day_data in user_data.items():\n",
        "                    print(f\"\\n{day_id}:\")\n",
        "                    print(f\"  Visits: {day_data['total_visits']}\")\n",
        "                    print(f\"  Narrative: {day_data['full_narrative'][:100]}...\")\n",
        "            else:\n",
        "                print(f\"No data for {user_id} in {period_type}\")\n",
        "    else:\n",
        "        # Old structure\n",
        "        if user_id in corpus:\n",
        "            user_data = corpus[user_id]\n",
        "            print(f\"Total days: {len(user_data)}\")\n",
        "            \n",
        "            for day_id, day_data in user_data.items():\n",
        "                print(f\"\\n{day_id}:\")\n",
        "                print(f\"  Visits: {day_data['total_visits']}\")\n",
        "                print(f\"  Narrative: {day_data['full_narrative'][:100]}...\")\n",
        "        else:\n",
        "            print(f\"User {user_id} not found\")"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1754451217174
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_outputs(df, num_users=3):\n",
        "    \"\"\"Create sample outputs to demonstrate the approach\"\"\"\n",
        "    \n",
        "    sample_users = df['user_id'].unique()[:num_users]\n",
        "    \n",
        "    for user_id in sample_users:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"SAMPLE OUTPUT FOR USER {user_id}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        user_data = df[df['user_id'] == user_id]\n",
        "        sample_day = user_data['day'].iloc[0]\n",
        "        day_data = user_data[user_data['day'] == sample_day]\n",
        "        \n",
        "        # Show different narrative styles\n",
        "        for style in ['simple', 'medium', 'detailed']:\n",
        "            narrative = create_daily_narrative(day_data, style)\n",
        "            print(f\"\\n{style.upper()} STYLE:\")\n",
        "            print(f\"{narrative['full_narrative']}\")\n",
        "        \n",
        "        print(f\"\\nCOORDINATE SEQUENCE:\")\n",
        "        coords = [(row['grid_x'], row['grid_y']) for _, row in day_data.iterrows()]\n",
        "        print(f\"Grid coordinates: {coords}\")"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1754451217327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Demonstrate text-to-coordinate mapping for period-separated data\n",
        "def demonstrate_text_to_coordinate_mapping_updated(corpus, user_id='user_0', day='day_0'):\n",
        "    \"\"\"Demonstrate how text maps back to coordinates for period-separated data\"\"\"\n",
        "    \n",
        "    print(f\"\\nTEXT-TO-COORDINATE MAPPING:\")\n",
        "    \n",
        "    # Handle new period-separated structure\n",
        "    if 'normal_period' in corpus and 'emergency_period' in corpus:\n",
        "        found = False\n",
        "        for period_type in ['normal_period', 'emergency_period']:\n",
        "            if user_id in corpus[period_type] and day in corpus[period_type][user_id]:\n",
        "                day_data = corpus[period_type][user_id][day]\n",
        "                found = True\n",
        "                \n",
        "                print(f\"Period: {period_type.replace('_', ' ').title()}\")\n",
        "                print(f\"Narrative: {day_data['full_narrative']}\")\n",
        "                print(f\"\\nBreakdown:\")\n",
        "                \n",
        "                for i, (sentence, context) in enumerate(zip(\n",
        "                    day_data['individual_sentences'], \n",
        "                    day_data['sentence_contexts']\n",
        "                )):\n",
        "                    coords = context['metadata']['grid_coords']\n",
        "                    timestamp = context['metadata']['timestamp']\n",
        "                    \n",
        "                    print(f\"{i+1}. {sentence}\")\n",
        "                    print(f\"   → Coordinates: {coords}, Time: {timestamp}\")\n",
        "                \n",
        "                break\n",
        "        \n",
        "        if not found:\n",
        "            print(f\"Data not found for {user_id} on {day}\")\n",
        "    else:\n",
        "        # Old structure\n",
        "        if user_id in corpus and day in corpus[user_id]:\n",
        "            day_data = corpus[user_id][day]\n",
        "            \n",
        "            print(f\"Narrative: {day_data['full_narrative']}\")\n",
        "            print(f\"\\nBreakdown:\")\n",
        "            \n",
        "            for i, (sentence, context) in enumerate(zip(\n",
        "                day_data['individual_sentences'], \n",
        "                day_data['sentence_contexts']\n",
        "            )):\n",
        "                coords = context['metadata']['grid_coords']\n",
        "                timestamp = context['metadata']['timestamp']\n",
        "                \n",
        "                print(f\"{i+1}. {sentence}\")\n",
        "                print(f\"   → Coordinates: {coords}, Time: {timestamp}\")\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1754451217472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW FUNCTION: Analyze patterns by period\n",
        "def analyze_patterns_by_period(corpus):\n",
        "    \"\"\"Analyze patterns specifically comparing normal vs emergency periods\"\"\"\n",
        "    \n",
        "    if 'normal_period' not in corpus or 'emergency_period' not in corpus:\n",
        "        print(\"No period separation found in corpus\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\n=== PERIOD COMPARISON ANALYSIS ===\")\n",
        "    \n",
        "    for period_type in ['normal_period', 'emergency_period']:\n",
        "        print(f\"\\n--- {period_type.upper().replace('_', ' ')} ---\")\n",
        "        \n",
        "        period_data = corpus[period_type]\n",
        "        total_users = len(period_data)\n",
        "        total_days = sum(len(user_data) for user_data in period_data.values())\n",
        "        total_visits = sum(\n",
        "            day_data['total_visits'] \n",
        "            for user_data in period_data.values() \n",
        "            for day_data in user_data.values()\n",
        "        )\n",
        "        \n",
        "        print(f\"Users: {total_users}\")\n",
        "        print(f\"Total days: {total_days}\")\n",
        "        print(f\"Total visits: {total_visits}\")\n",
        "        print(f\"Avg visits per day: {total_visits/total_days:.1f}\" if total_days > 0 else \"No data\")\n",
        "        \n",
        "        # Extract activities for this period\n",
        "        activities = []\n",
        "        for user_data in period_data.values():\n",
        "            for day_data in user_data.values():\n",
        "                if 'sentence_contexts' in day_data:\n",
        "                    for context in day_data['sentence_contexts']:\n",
        "                        activity = context['contexts']['activity']['inferred_activity']\n",
        "                        activities.append(activity)\n",
        "        \n",
        "        # Show top activities\n",
        "        from collections import Counter\n",
        "        if activities:\n",
        "            top_activities = Counter(activities).most_common(5)\n",
        "            print(\"Top activities:\")\n",
        "            for activity, count in top_activities:\n",
        "                print(f\"  {activity}: {count}\")\n",
        "\n",
        "def prepare_for_bert_training(corpus, mask_probability=0.15):\n",
        "    \"\"\"Prepare data for BERT-style masked language model training\"\"\"\n",
        "    \n",
        "    bert_training_data = []\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in corpus and 'emergency_period' in corpus:\n",
        "        for period_data in corpus.values():\n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    sentences = day_data['individual_sentences']\n",
        "                    \n",
        "                    # Create masked versions for BERT training\n",
        "                    for mask_idx in range(len(sentences)):\n",
        "                        if np.random.random() < mask_probability:\n",
        "                            masked_sentences = sentences.copy()\n",
        "                            original_sentence = masked_sentences[mask_idx]\n",
        "                            masked_sentences[mask_idx] = \"[MASK]\"\n",
        "                            \n",
        "                            bert_training_data.append({\n",
        "                                'input': ' '.join(masked_sentences),\n",
        "                                'target': original_sentence,\n",
        "                                'mask_position': mask_idx\n",
        "                            })\n",
        "    else:\n",
        "        # Old structure\n",
        "        for user_data in corpus.values():\n",
        "            for day_data in user_data.values():\n",
        "                sentences = day_data['individual_sentences']\n",
        "                \n",
        "                # Create masked versions for BERT training\n",
        "                for mask_idx in range(len(sentences)):\n",
        "                    if np.random.random() < mask_probability:\n",
        "                        masked_sentences = sentences.copy()\n",
        "                        original_sentence = masked_sentences[mask_idx]\n",
        "                        masked_sentences[mask_idx] = \"[MASK]\"\n",
        "                        \n",
        "                        bert_training_data.append({\n",
        "                            'input': ' '.join(masked_sentences),\n",
        "                            'target': original_sentence,\n",
        "                            'mask_position': mask_idx\n",
        "                        })\n",
        "    \n",
        "    return bert_training_data\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1754451217636
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED MAIN FUNCTION: For regular mobility with period separation\n",
        "def main_with_period_separation():\n",
        "    \"\"\"Main execution pipeline with period separation\"\"\"\n",
        "    \n",
        "    print(\"Creating folder structure...\")\n",
        "    base_path = create_user_folders()\n",
        "    \n",
        "    print(\"Loading mobility datasets...\")\n",
        "    df = load_mobility_datasets()\n",
        "    print(f\"Loaded {len(df)} mobility records for {df['user_id'].nunique()} users\")\n",
        "    \n",
        "    # Check period distribution\n",
        "    normal_days = df[df['day'] <= 60]['day'].nunique()\n",
        "    emergency_days = df[(df['day'] > 60) & (df['day'] <= 75)]['day'].nunique()\n",
        "    \n",
        "    print(f\"Normal period: {normal_days} days (0-60)\")\n",
        "    print(f\"Emergency period: {emergency_days} days (61-75)\")\n",
        "    \n",
        "    print(\"\\nGenerating mobility text corpus by period...\")\n",
        "    \n",
        "    # Generate different narrative styles\n",
        "    styles = ['simple', 'medium', 'detailed']\n",
        "    \n",
        "    for style in styles:\n",
        "        print(f\"\\nProcessing {style} narrative style...\")\n",
        "        \n",
        "        # Generate corpus separated by period\n",
        "        corpus_by_period = generate_mobility_corpus_by_period(df, output_style=style)\n",
        "        \n",
        "        # Create style-specific folder\n",
        "        style_path = os.path.join(base_path, f'{style}_style')\n",
        "        if not os.path.exists(style_path):\n",
        "            os.makedirs(style_path)\n",
        "            # Create period subfolders\n",
        "            for period in ['normal_period', 'emergency_period']:\n",
        "                period_path = os.path.join(style_path, period)\n",
        "                if not os.path.exists(period_path):\n",
        "                    os.makedirs(period_path)\n",
        "        \n",
        "        # Save individual user files\n",
        "        save_user_files_by_period(corpus_by_period, style_path)\n",
        "        \n",
        "        # Export training data by period\n",
        "        export_training_data_by_period(corpus_by_period, style_path)\n",
        "        \n",
        "        print(f\"  Normal period: {len(corpus_by_period['normal_period'])} users\")\n",
        "        print(f\"  Emergency period: {len(corpus_by_period['emergency_period'])} users\")\n",
        "    \n",
        "    # Create user summaries (using medium style)\n",
        "    print(\"\\nCreating user summary files...\")\n",
        "    medium_corpus = generate_mobility_corpus_by_period(df, output_style='medium')\n",
        "    create_user_summary_files(medium_corpus, base_path)\n",
        "    \n",
        "    # Create overall statistics\n",
        "    stats_file = os.path.join(base_path, 'dataset_statistics.txt')\n",
        "    with open(stats_file, 'w') as f:\n",
        "        f.write(\"=== MOBILITY DATASET STATISTICS ===\\n\\n\")\n",
        "        f.write(f\"Total users: {df['user_id'].nunique()}\\n\")\n",
        "        f.write(f\"Total records: {len(df)}\\n\")\n",
        "        f.write(f\"Total days: {df['day'].nunique()}\\n\")\n",
        "        f.write(f\"Normal period days: 0-60 ({normal_days} days)\\n\")\n",
        "        f.write(f\"Emergency period days: 61-75 ({emergency_days} days)\\n\\n\")\n",
        "        \n",
        "        # Period-specific stats\n",
        "        for period_name, period_filter in [('Normal', df['day'] <= 60), \n",
        "                                          ('Emergency', (df['day'] > 60) & (df['day'] <= 75))]:\n",
        "            period_data = df[period_filter]\n",
        "            f.write(f\"{period_name} Period:\\n\")\n",
        "            f.write(f\"  Users: {period_data['user_id'].nunique()}\\n\")\n",
        "            f.write(f\"  Records: {len(period_data)}\\n\")\n",
        "            f.write(f\"  Avg records per user: {len(period_data) / period_data['user_id'].nunique():.1f}\\n\\n\")\n",
        "    \n",
        "    print(f\"\\n Period-separated mobility text generation completed!\")\n",
        "    print(f\" All files saved in: {base_path}\")\n",
        "    \n",
        "    return medium_corpus, base_path\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1754451217790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED ANALYSIS FUNCTION\n",
        "def run_analysis_with_period_separation(corpus):\n",
        "    \"\"\"Run all analysis functions with period separation support\"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"MOBILITY PATTERNS ANALYSIS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Extract patterns (updated function)\n",
        "    patterns = extract_mobility_patterns(corpus)\n",
        "    for pattern_type, activities in patterns.items():\n",
        "        if activities:  # Only show patterns with data\n",
        "            print(f\"\\n{pattern_type.upper()}:\")\n",
        "            for activity, count in activities[:5]:\n",
        "                print(f\"  {activity}: {count}\")\n",
        "    \n",
        "    # Period-specific analysis\n",
        "    analyze_patterns_by_period(corpus)\n",
        "    \n",
        "    # Build vocabulary (updated function)\n",
        "    vocabulary = build_mobility_vocabulary_updated(corpus)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"VOCABULARY ANALYSIS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    for vocab_type, words in vocabulary.items():\n",
        "        print(f\"{vocab_type}: {len(words)} unique terms\")\n",
        "    \n",
        "    # User analysis (updated function)\n",
        "    analyze_user_narratives_updated(corpus, 'user_0')\n",
        "    \n",
        "    # Text-to-coordinate mapping (updated function)\n",
        "    demonstrate_text_to_coordinate_mapping_updated(corpus, 'user_0', 'day_5')\n",
        "    \n",
        "    return patterns, vocabulary"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1754451217938
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POI Binary Matrix Integration Functions\n",
        "\n",
        "def load_poi_presence_matrix():\n",
        "    \"\"\"Load and process the POI presence matrix\"\"\"\n",
        "    poi_df = pd.read_csv(\"scientific_mobility_poi_matrix_dataset.csv\")\n",
        "    \n",
        "    # Identify POI binary columns\n",
        "    poi_binary_cols = [col for col in poi_df.columns if col.startswith('has_')]\n",
        "    function_binary_cols = [col for col in poi_df.columns if 'function' in col.lower()]\n",
        "    \n",
        "    return poi_df, poi_binary_cols, function_binary_cols\n",
        "\n",
        "def merge_with_poi_features(mobility_df, poi_df):\n",
        "    \"\"\"Merge mobility data with POI binary features\"\"\"\n",
        "    \n",
        "    if mobility_df is None:\n",
        "        print(\"ERROR: mobility_df is None\")\n",
        "        return None\n",
        "    if poi_df is None:\n",
        "        print(\"ERROR: poi_df is None\")\n",
        "        return mobility_df\n",
        "    \n",
        "    print(f\"Mobility DF shape: {mobility_df.shape}\")\n",
        "    print(f\"POI DF shape: {poi_df.shape}\")\n",
        "    \n",
        "    # Remove duplicate columns from POI data before merging\n",
        "    poi_columns_to_keep = [col for col in poi_df.columns if col not in mobility_df.columns]\n",
        "    poi_df_clean = poi_df[poi_columns_to_keep]\n",
        "    \n",
        "    print(f\"POI columns to add: {len(poi_columns_to_keep)}\")\n",
        "    \n",
        "    if len(mobility_df) == len(poi_df_clean):\n",
        "        print(\"Same length - merging by index\")\n",
        "        merged_df = pd.concat([mobility_df.reset_index(drop=True), \n",
        "                              poi_df_clean.reset_index(drop=True)], axis=1)\n",
        "    else:\n",
        "        print(\"Different lengths - using mobility data only\")\n",
        "        merged_df = mobility_df\n",
        "    \n",
        "    print(f\"Merged DataFrame shape: {merged_df.shape}\")\n",
        "    return merged_df"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1754451218082
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_active_pois(row, poi_binary_cols):\n",
        "    \"\"\"Extract active POI features for a specific record\"\"\"\n",
        "    \n",
        "    active_pois = []\n",
        "    for col in poi_binary_cols:\n",
        "        if col in row and row[col] == 1:\n",
        "            # Clean up POI name\n",
        "            poi_name = col.replace('has_', '').replace('_', ' ')\n",
        "            active_pois.append(poi_name)\n",
        "    \n",
        "    return active_pois"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1754451218225
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poi_enriched_context(row, poi_binary_cols, function_binary_cols):\n",
        "    \"\"\"Generate context enriched with POI binary features\"\"\"\n",
        "    \n",
        "    active_pois = extract_active_pois(row, poi_binary_cols)\n",
        "    active_functions = extract_active_pois(row, function_binary_cols)\n",
        "    \n",
        "    # Create POI context descriptions\n",
        "    poi_context = {\n",
        "        'nearby_pois': active_pois[:5],  # Limit to top 5 for readability\n",
        "        'available_functions': active_functions[:3],\n",
        "        'poi_richness': len(active_pois),\n",
        "        'functional_diversity': len(active_functions)\n",
        "    }\n",
        "    \n",
        "    # Generate natural language descriptions\n",
        "    if active_pois:\n",
        "        if len(active_pois) == 1:\n",
        "            poi_desc = f\"near {active_pois[0]}\"\n",
        "        elif len(active_pois) <= 3:\n",
        "            poi_desc = f\"near {', '.join(active_pois[:-1])} and {active_pois[-1]}\"\n",
        "        else:\n",
        "            poi_desc = f\"in area with {active_pois[0]}, {active_pois[1]} and {len(active_pois)-2} other amenities\"\n",
        "    else:\n",
        "        poi_desc = \"in area with basic amenities\"\n",
        "    \n",
        "    if active_functions:\n",
        "        function_desc = f\"offering {', '.join(active_functions)}\"\n",
        "    else:\n",
        "        function_desc = \"with standard services\"\n",
        "    \n",
        "    poi_context.update({\n",
        "        'poi_description': poi_desc,\n",
        "        'function_description': function_desc,\n",
        "        'full_poi_context': f\"{poi_desc} {function_desc}\"\n",
        "    })\n",
        "    \n",
        "    return poi_context"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1754451218365
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_poi_enriched_sentence(row, poi_binary_cols, function_binary_cols):\n",
        "    \"\"\"Create mobility sentence enriched with POI information\"\"\"\n",
        "    \n",
        "    # Generate base contexts\n",
        "    time_ctx = generate_time_context(\n",
        "        row['hour'], row['minute'], \n",
        "        row['day_name'], row['is_weekend'] == 'True'\n",
        "    )\n",
        "    \n",
        "    location_ctx = generate_location_context(\n",
        "        row['location_category'], row['location_function'],\n",
        "        row['poi_density'], row['category_diversity'],\n",
        "        row['distance_from_center'], row['distance_quartile']\n",
        "    )\n",
        "    \n",
        "    activity_ctx = generate_activity_context(\n",
        "        row.get('functional_category', None),\n",
        "        row['location_function'],\n",
        "        time_ctx['period']\n",
        "    )\n",
        "    \n",
        "    # Generate POI-enriched context\n",
        "    poi_ctx = generate_poi_enriched_context(row, poi_binary_cols, function_binary_cols)\n",
        "    \n",
        "    # Create enhanced sentences\n",
        "    enhanced_sentences = {\n",
        "        'poi_detailed': f\"{time_ctx['full_context']}, the user visited {row['location_category']} {poi_ctx['full_poi_context']} and {activity_ctx['context']}.\",\n",
        "        \n",
        "        'poi_medium': f\"At {time_ctx['time_str']}, user was at {row['location_category']} {poi_ctx['poi_description']} {activity_ctx['context']}.\",\n",
        "        \n",
        "        'poi_simple': f\"{time_ctx['period']}: {row['location_category']} {poi_ctx['poi_description']} - {activity_ctx['inferred_activity']}\",\n",
        "        \n",
        "        'poi_rich': f\"During {time_ctx['day_context']} at {time_ctx['time_str']}, user engaged in {activity_ctx['inferred_activity']} at {row['location_category']} in {location_ctx['density_desc']} {poi_ctx['full_poi_context']}.\"\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'sentences': enhanced_sentences,\n",
        "        'contexts': {\n",
        "            'time': time_ctx,\n",
        "            'location': location_ctx,\n",
        "            'activity': activity_ctx,\n",
        "            'poi': poi_ctx\n",
        "        },\n",
        "        'metadata': {\n",
        "            'grid_coords': (row['grid_x'], row['grid_y']),\n",
        "            'timestamp': f\"Day_{row['day']}_Slot_{row['time_slot']}\",\n",
        "            'poi_count': poi_ctx['poi_richness'],\n",
        "            'function_count': poi_ctx['functional_diversity']\n",
        "        }\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1754451218510
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poi_vocabulary(merged_df, poi_binary_cols, function_binary_cols):\n",
        "    \"\"\"Generate vocabulary from POI features\"\"\"\n",
        "    \n",
        "    poi_vocab = {\n",
        "        'poi_types': set(),\n",
        "        'function_types': set(),\n",
        "        'poi_combinations': set()\n",
        "    }\n",
        "    \n",
        "    for _, row in merged_df.iterrows():\n",
        "        active_pois = extract_active_pois(row, poi_binary_cols)\n",
        "        active_functions = extract_active_pois(row, function_binary_cols)\n",
        "        \n",
        "        poi_vocab['poi_types'].update(active_pois)\n",
        "        poi_vocab['function_types'].update(active_functions)\n",
        "        \n",
        "        # Create combinations for common patterns\n",
        "        if len(active_pois) >= 2:\n",
        "            poi_combo = ' + '.join(sorted(active_pois[:3]))\n",
        "            poi_vocab['poi_combinations'].add(poi_combo)\n",
        "    \n",
        "    # Convert to sorted lists\n",
        "    for key in poi_vocab:\n",
        "        poi_vocab[key] = sorted(list(poi_vocab[key]))\n",
        "    \n",
        "    return poi_vocab\n"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1754451218654
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED: Create POI-enhanced corpus separated by periods\n",
        "def create_poi_enhanced_corpus_by_period(df_with_poi, poi_binary_cols, function_binary_cols, style='poi_medium'):\n",
        "    \"\"\"Create POI-enhanced corpus separated by periods\"\"\"\n",
        "    \n",
        "    enhanced_corpus = {\n",
        "        'normal_period': {},\n",
        "        'emergency_period': {}\n",
        "    }\n",
        "    \n",
        "    if df_with_poi is None:\n",
        "        print(\"ERROR: df_with_poi is None\")\n",
        "        return enhanced_corpus\n",
        "    \n",
        "    # Handle duplicate column issue\n",
        "    if 'user_id' in df_with_poi.columns:\n",
        "        user_id_series = df_with_poi.iloc[:, df_with_poi.columns.get_loc('user_id')]\n",
        "        if hasattr(user_id_series, 'unique'):\n",
        "            unique_users = user_id_series.unique()\n",
        "        else:\n",
        "            unique_users = user_id_series.iloc[:, 0].unique()\n",
        "    else:\n",
        "        print(f\"ERROR: 'user_id' column not found\")\n",
        "        return enhanced_corpus\n",
        "    \n",
        "    for user_id in unique_users:\n",
        "        user_mask = (df_with_poi.iloc[:, df_with_poi.columns.get_loc('user_id')] == user_id)\n",
        "        user_data = df_with_poi[user_mask]\n",
        "        \n",
        "        # Initialize user data for both periods\n",
        "        enhanced_corpus['normal_period'][f'user_{user_id}'] = {}\n",
        "        enhanced_corpus['emergency_period'][f'user_{user_id}'] = {}\n",
        "        \n",
        "        for day in user_data['day'].unique():\n",
        "            day_data = user_data[user_data['day'] == day]\n",
        "            \n",
        "            if len(day_data) > 0:\n",
        "                # Determine period type\n",
        "                period_type = get_period_type(day)\n",
        "                \n",
        "                if period_type in ['normal', 'emergency']:\n",
        "                    period_key = f'{period_type}_period'\n",
        "                    \n",
        "                    daily_sentences = []\n",
        "                    contexts = []\n",
        "                    \n",
        "                    day_data_sorted = day_data.sort_values('time_slot')\n",
        "                    \n",
        "                    for _, row in day_data_sorted.iterrows():\n",
        "                        sentence_data = create_poi_enriched_sentence(\n",
        "                            row, poi_binary_cols, function_binary_cols\n",
        "                        )\n",
        "                        daily_sentences.append(sentence_data['sentences'][style])\n",
        "                        contexts.append(sentence_data)\n",
        "                    \n",
        "                    # Create narrative with period info\n",
        "                    narrative_parts = []\n",
        "                    for i, sentence in enumerate(daily_sentences):\n",
        "                        if i == 0:\n",
        "                            narrative_parts.append(f\"The day began: {sentence}\")\n",
        "                        elif i == len(daily_sentences) - 1:\n",
        "                            narrative_parts.append(f\"Finally, {sentence}\")\n",
        "                        else:\n",
        "                            connectors = [\"Then,\", \"Next,\", \"Later,\", \"Subsequently,\"]\n",
        "                            connector = connectors[i % len(connectors)]\n",
        "                            narrative_parts.append(f\"{connector} {sentence}\")\n",
        "                    \n",
        "                    enhanced_corpus[period_key][f'user_{user_id}'][f'day_{day}'] = {\n",
        "                        'full_narrative': ' '.join(narrative_parts),\n",
        "                        'individual_sentences': daily_sentences,\n",
        "                        'sentence_contexts': contexts,\n",
        "                        'total_visits': len(daily_sentences),\n",
        "                        'period_type': period_type,\n",
        "                        'day_number': day,\n",
        "                        'poi_richness': sum(ctx['metadata']['poi_count'] for ctx in contexts),\n",
        "                        'avg_poi_per_visit': sum(ctx['metadata']['poi_count'] for ctx in contexts) / len(contexts) if contexts else 0\n",
        "                    }\n",
        "    \n",
        "    return enhanced_corpus"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1754451218797
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_poi_patterns(enhanced_corpus):\n",
        "    \"\"\"Analyze POI usage patterns from the enhanced corpus\"\"\"\n",
        "    \n",
        "    poi_patterns = {\n",
        "        'poi_sequences': [],\n",
        "        'time_poi_associations': defaultdict(list),\n",
        "        'activity_poi_associations': defaultdict(list),\n",
        "        'poi_diversity_by_user': {}\n",
        "    }\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in enhanced_corpus and 'emergency_period' in enhanced_corpus:\n",
        "        for period_data in enhanced_corpus.values():\n",
        "            for user_id, user_data in period_data.items():\n",
        "                user_pois = set()\n",
        "                \n",
        "                for day_data in user_data.values():\n",
        "                    day_poi_sequence = []\n",
        "                    \n",
        "                    if 'sentence_contexts' in day_data:\n",
        "                        for context in day_data['sentence_contexts']:\n",
        "                            poi_ctx = context['contexts']['poi']\n",
        "                            time_ctx = context['contexts']['time']\n",
        "                            activity_ctx = context['contexts']['activity']\n",
        "                            \n",
        "                            # Collect POI sequences\n",
        "                            if poi_ctx['nearby_pois']:\n",
        "                                day_poi_sequence.extend(poi_ctx['nearby_pois'])\n",
        "                                user_pois.update(poi_ctx['nearby_pois'])\n",
        "                                \n",
        "                                # Time-POI associations\n",
        "                                for poi in poi_ctx['nearby_pois']:\n",
        "                                    poi_patterns['time_poi_associations'][time_ctx['period']].append(poi)\n",
        "                                    poi_patterns['activity_poi_associations'][activity_ctx['inferred_activity']].append(poi)\n",
        "                    \n",
        "                    if day_poi_sequence:\n",
        "                        poi_patterns['poi_sequences'].append(day_poi_sequence)\n",
        "                \n",
        "                poi_patterns['poi_diversity_by_user'][user_id] = len(user_pois)\n",
        "    \n",
        "    # Count frequencies\n",
        "    from collections import Counter\n",
        "    \n",
        "    for pattern_type in ['time_poi_associations', 'activity_poi_associations']:\n",
        "        for key, values in poi_patterns[pattern_type].items():\n",
        "            poi_patterns[pattern_type][key] = Counter(values).most_common(10)\n",
        "    \n",
        "    return poi_patterns"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1754451218949
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_trajectory_templates(enhanced_corpus):\n",
        "    \"\"\"Create trajectory templates based on POI patterns\"\"\"\n",
        "    \n",
        "    templates = {\n",
        "        'morning_routines': [],\n",
        "        'evening_routines': [],\n",
        "        'weekend_patterns': [],\n",
        "        'workday_patterns': []\n",
        "    }\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in enhanced_corpus and 'emergency_period' in enhanced_corpus:\n",
        "        for period_data in enhanced_corpus.values():\n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    if 'sentence_contexts' not in day_data:\n",
        "                        continue\n",
        "                        \n",
        "                    day_template = {\n",
        "                        'early morning': [],\n",
        "                        'morning': [],\n",
        "                        'afternoon': [],\n",
        "                        'evening': [],\n",
        "                        'night': [],\n",
        "                        'poi_flow': [],\n",
        "                        'activity_flow': []\n",
        "                    }\n",
        "                    \n",
        "                    for context in day_data['sentence_contexts']:\n",
        "                        time_period = context['contexts']['time']['period']\n",
        "                        activity = context['contexts']['activity']['inferred_activity']\n",
        "                        pois = context['contexts']['poi']['nearby_pois']\n",
        "                        \n",
        "                        # Handle time period mapping\n",
        "                        if time_period in day_template:\n",
        "                            day_template[time_period].append({\n",
        "                                'activity': activity,\n",
        "                                'pois': pois,\n",
        "                                'location': context['contexts']['location']['primary_location']\n",
        "                            })\n",
        "                        else:\n",
        "                            # Map unexpected time periods\n",
        "                            if 'early' in time_period.lower():\n",
        "                                mapped_period = 'early morning'\n",
        "                            elif 'night' in time_period.lower():\n",
        "                                mapped_period = 'night'\n",
        "                            else:\n",
        "                                mapped_period = 'morning'  # Default fallback\n",
        "                            \n",
        "                            day_template[mapped_period].append({\n",
        "                                'activity': activity,\n",
        "                                'pois': pois,\n",
        "                                'location': context['contexts']['location']['primary_location']\n",
        "                            })\n",
        "                        \n",
        "                        day_template['poi_flow'].extend(pois)\n",
        "                        day_template['activity_flow'].append(activity)\n",
        "                    \n",
        "                    # Categorize templates\n",
        "                    is_weekend = any('weekend' in ctx['contexts']['time']['day_context'] \n",
        "                                   for ctx in day_data['sentence_contexts'])\n",
        "                    \n",
        "                    if is_weekend:\n",
        "                        templates['weekend_patterns'].append(day_template)\n",
        "                    else:\n",
        "                        templates['workday_patterns'].append(day_template)\n",
        "                    \n",
        "                    # Combine early morning and morning for routines\n",
        "                    morning_activities = day_template['early morning'] + day_template['morning']\n",
        "                    if morning_activities:\n",
        "                        templates['morning_routines'].append(morning_activities)\n",
        "                    \n",
        "                    evening_activities = day_template['evening'] + day_template['night']\n",
        "                    if evening_activities:\n",
        "                        templates['evening_routines'].append(evening_activities)\n",
        "    \n",
        "    return templates"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1754451219097
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_bert_training_with_poi(enhanced_corpus, mask_poi_probability=0.2):\n",
        "    \"\"\"Prepare BERT training data with POI masking\"\"\"\n",
        "    \n",
        "    bert_poi_training = []\n",
        "    \n",
        "    # Handle period-separated structure\n",
        "    if 'normal_period' in enhanced_corpus and 'emergency_period' in enhanced_corpus:\n",
        "        for period_data in enhanced_corpus.values():\n",
        "            for user_data in period_data.values():\n",
        "                for day_data in user_data.values():\n",
        "                    if 'individual_sentences' not in day_data or 'sentence_contexts' not in day_data:\n",
        "                        continue\n",
        "                        \n",
        "                    sentences = day_data['individual_sentences']\n",
        "                    contexts = day_data['sentence_contexts']\n",
        "                    \n",
        "                    for i, (sentence, context) in enumerate(zip(sentences, contexts)):\n",
        "                        # Standard sentence masking\n",
        "                        if np.random.random() < 0.15:\n",
        "                            masked_sentences = sentences.copy()\n",
        "                            masked_sentences[i] = \"[MASK]\"\n",
        "                            \n",
        "                            bert_poi_training.append({\n",
        "                                'input': ' '.join(masked_sentences),\n",
        "                                'target': sentence,\n",
        "                                'mask_type': 'full_sentence',\n",
        "                                'poi_count': context['metadata']['poi_count']\n",
        "                            })\n",
        "                        \n",
        "                        # POI-specific masking\n",
        "                        if context['contexts']['poi']['nearby_pois'] and np.random.random() < mask_poi_probability:\n",
        "                            # Mask POI information in sentence\n",
        "                            masked_sentence = sentence\n",
        "                            for poi in context['contexts']['poi']['nearby_pois']:\n",
        "                                masked_sentence = masked_sentence.replace(poi, '[POI_MASK]')\n",
        "                            \n",
        "                            bert_poi_training.append({\n",
        "                                'input': masked_sentence,\n",
        "                                'target': sentence,\n",
        "                                'mask_type': 'poi_specific',\n",
        "                                'masked_pois': context['contexts']['poi']['nearby_pois']\n",
        "                            })\n",
        "    \n",
        "    return bert_poi_training\n"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1754451219242
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_trajectories(templates, poi_vocab, num_trajectories=10):\n",
        "    \"\"\"Generate synthetic trajectories using learned templates\"\"\"\n",
        "    \n",
        "    synthetic_trajectories = []\n",
        "    \n",
        "    for i in range(num_trajectories):\n",
        "        # Check if templates exist before choosing\n",
        "        available_template_types = []\n",
        "        if templates['workday_patterns']:\n",
        "            available_template_types.append('workday_patterns')\n",
        "        if templates['weekend_patterns']:\n",
        "            available_template_types.append('weekend_patterns')\n",
        "        \n",
        "        if not available_template_types:\n",
        "            print(\"WARNING: No templates available, skipping synthetic trajectory generation\")\n",
        "            break\n",
        "        \n",
        "        # Choose random template type from available ones\n",
        "        template_type = np.random.choice(available_template_types)\n",
        "        \n",
        "        # Check if template list is not empty\n",
        "        if not templates[template_type]:\n",
        "            print(f\"WARNING: No templates for {template_type}, skipping\")\n",
        "            continue\n",
        "            \n",
        "        template = np.random.choice(templates[template_type])\n",
        "        \n",
        "        synthetic_day = {\n",
        "            'synthetic_id': f'synth_{i}',\n",
        "            'template_type': template_type,\n",
        "            'generated_sentences': [],\n",
        "            'poi_sequence': []\n",
        "        }\n",
        "        \n",
        "        # Generate sentences for each time period\n",
        "        for period in ['early morning', 'morning', 'afternoon', 'evening', 'night']:\n",
        "            if period in template and template[period]:\n",
        "                period_activities = template[period]\n",
        "                \n",
        "                for activity_data in period_activities:\n",
        "                    # Use original or sample new POIs\n",
        "                    if activity_data['pois']:\n",
        "                        selected_pois = activity_data['pois']\n",
        "                    else:\n",
        "                        # Sample random POIs from vocabulary\n",
        "                        if poi_vocab['poi_types']:\n",
        "                            available_pois = poi_vocab['poi_types']\n",
        "                            num_pois = min(3, len(available_pois))\n",
        "                            selected_pois = np.random.choice(available_pois, \n",
        "                                                           size=num_pois, \n",
        "                                                           replace=False).tolist()\n",
        "                        else:\n",
        "                            selected_pois = []\n",
        "                    \n",
        "                    # Generate synthetic sentence\n",
        "                    time_desc = period\n",
        "                    activity = activity_data['activity']\n",
        "                    location = activity_data['location']\n",
        "                    poi_desc = f\"near {', '.join(selected_pois[:2])}\" if selected_pois else \"in area\"\n",
        "                    \n",
        "                    synthetic_sentence = f\"During {time_desc}, user {activity} at {location} {poi_desc}.\"\n",
        "                    \n",
        "                    synthetic_day['generated_sentences'].append(synthetic_sentence)\n",
        "                    synthetic_day['poi_sequence'].extend(selected_pois)\n",
        "        \n",
        "        # Only add if we generated some sentences\n",
        "        if synthetic_day['generated_sentences']:\n",
        "            synthetic_trajectories.append(synthetic_day)\n",
        "    \n",
        "    print(f\"Generated {len(synthetic_trajectories)} synthetic trajectories\")\n",
        "    return synthetic_trajectories"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1754451219390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_for_gpt_training(enhanced_corpus, output_file='Users/agugire/gpt_mobility_training.txt'):\n",
        "    \"\"\"Export data in format suitable for GPT-style autoregressive training\"\"\"\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        # Handle period-separated structure\n",
        "        if 'normal_period' in enhanced_corpus and 'emergency_period' in enhanced_corpus:\n",
        "            for period_type, period_data in enhanced_corpus.items():\n",
        "                f.write(f\"=== {period_type.upper().replace('_', ' ')} ===\\n\\n\")\n",
        "                \n",
        "                for user_data in period_data.values():\n",
        "                    for day_data in user_data.values():\n",
        "                        # Write full narrative\n",
        "                        f.write(day_data['full_narrative'] + '\\n')\n",
        "                        \n",
        "                        # Write individual sentences for fine-grained training\n",
        "                        for sentence in day_data['individual_sentences']:\n",
        "                            f.write(sentence + '\\n')\n",
        "                        \n",
        "                        f.write('\\n')  # Separator between days\n",
        "        else:\n",
        "            # Old structure\n",
        "            for user_data in enhanced_corpus.values():\n",
        "                for day_data in user_data.values():\n",
        "                    # Write full narrative\n",
        "                    f.write(day_data['full_narrative'] + '\\n')\n",
        "                    \n",
        "                    # Write individual sentences for fine-grained training\n",
        "                    for sentence in day_data['individual_sentences']:\n",
        "                        f.write(sentence + '\\n')\n",
        "                    \n",
        "                    f.write('\\n')  # Separator between days\n",
        "    \n",
        "    print(f\"Exported GPT training data to {output_file}\")"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1754451219550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_poi_feature_embeddings(poi_vocab, embedding_dim=100):\n",
        "    \"\"\"Create embeddings for POI features\"\"\"\n",
        "    \n",
        "    poi_to_id = {}\n",
        "    id_to_poi = {}\n",
        "    \n",
        "    all_pois = poi_vocab['poi_types'] + poi_vocab['function_types']\n",
        "    \n",
        "    for i, poi in enumerate(all_pois):\n",
        "        poi_to_id[poi] = i\n",
        "        id_to_poi[i] = poi\n",
        "    \n",
        "    # Initialize random embeddings (in practice, you'd train these)\n",
        "    embeddings = np.random.randn(len(all_pois), embedding_dim)\n",
        "    \n",
        "    return {\n",
        "        'poi_to_id': poi_to_id,\n",
        "        'id_to_poi': id_to_poi,\n",
        "        'embeddings': embeddings,\n",
        "        'vocab_size': len(all_pois),\n",
        "        'embedding_dim': embedding_dim\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1754451219693
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_poi_pipeline_with_periods():\n",
        "    \"\"\"Main POI pipeline with period separation - Azure compatible\"\"\"\n",
        "    \n",
        "    print(\"Creating POI folder structure...\")\n",
        "    base_path = create_user_folders('POI_UserMobilityTexts')\n",
        "    \n",
        "    print(\"Loading datasets...\")\n",
        "    mobility_df = load_mobility_datasets()\n",
        "    \n",
        "    if mobility_df is None:\n",
        "        print(\"Cannot proceed without mobility datasets\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    poi_df, poi_binary_cols, function_binary_cols = load_poi_presence_matrix()\n",
        "    \n",
        "    print(\"Merging with POI features...\")\n",
        "    merged_df = merge_with_poi_features(mobility_df, poi_df)\n",
        "    \n",
        "    if merged_df is None:\n",
        "        merged_df = mobility_df\n",
        "        poi_binary_cols = []\n",
        "        function_binary_cols = []\n",
        "    \n",
        "    print(\"Generating POI-enhanced corpus by period...\")\n",
        "    enhanced_corpus = create_poi_enhanced_corpus_by_period(\n",
        "        merged_df, poi_binary_cols, function_binary_cols, style='poi_medium'\n",
        "    )\n",
        "    \n",
        "    # Save POI-enhanced data by period\n",
        "    save_user_files_by_period(enhanced_corpus, base_path)\n",
        "    export_training_data_by_period(enhanced_corpus, base_path)\n",
        "    create_user_summary_files(enhanced_corpus, base_path)\n",
        "    \n",
        "    # Build POI vocabulary\n",
        "    poi_vocab = generate_poi_vocabulary(merged_df, poi_binary_cols, function_binary_cols)\n",
        "    \n",
        "    # Analyze patterns\n",
        "    patterns = analyze_poi_patterns(enhanced_corpus)\n",
        "    \n",
        "    # Create templates\n",
        "    templates = create_trajectory_templates(enhanced_corpus)\n",
        "    \n",
        "    # Prepare BERT training data\n",
        "    bert_poi_data = prepare_bert_training_with_poi(enhanced_corpus)\n",
        "    \n",
        "    # Generate synthetic trajectories\n",
        "    synthetic_trajs = generate_synthetic_trajectories(templates, poi_vocab, num_trajectories=20)\n",
        "    \n",
        "    # Export additional POI data with Azure-compatible paths\n",
        "    with open(os.path.join(base_path, 'poi_vocabulary.json'), 'w') as f:\n",
        "        json.dump(poi_vocab, f, indent=2)\n",
        "    \n",
        "    with open(os.path.join(base_path, 'poi_patterns.json'), 'w') as f:\n",
        "        json.dump(patterns, f, indent=2, default=str)\n",
        "    \n",
        "    with open(os.path.join(base_path, 'trajectory_templates.json'), 'w') as f:\n",
        "        json.dump(templates, f, indent=2)\n",
        "    \n",
        "    with open(os.path.join(base_path, 'bert_poi_training.json'), 'w') as f:\n",
        "        json.dump(bert_poi_data, f, indent=2)\n",
        "    \n",
        "    with open(os.path.join(base_path, 'synthetic_trajectories.json'), 'w') as f:\n",
        "        json.dump(synthetic_trajs, f, indent=2)\n",
        "    \n",
        "    # Export for GPT training\n",
        "    export_for_gpt_training(enhanced_corpus, os.path.join(base_path, 'gpt_mobility_training.txt'))\n",
        "    \n",
        "    # Create POI embeddings\n",
        "    poi_embeddings = create_poi_feature_embeddings(poi_vocab)\n",
        "    np.save(os.path.join(base_path, 'poi_embeddings.npy'), poi_embeddings['embeddings'])\n",
        "    with open(os.path.join(base_path, 'poi_embedding_vocab.json'), 'w') as f:\n",
        "        json.dump({k: v for k, v in poi_embeddings.items() if k != 'embeddings'}, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n POI-enhanced period-separated pipeline completed!\")\n",
        "    print(f\" POI files saved in: {base_path}\")\n",
        "    \n",
        "    return enhanced_corpus, poi_vocab, patterns, templates"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1754451219842
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\" Starting Mobility Text Generation with Period Separation...\")\n",
        "    print(\"\\nChoose pipeline:\")\n",
        "    print(\"1. Regular mobility pipeline\")\n",
        "    print(\"2. POI-enhanced mobility pipeline\") \n",
        "    print(\"3. Both pipelines\")\n",
        "    \n",
        "    choice = input(\"Enter choice (1/2/3) or just press Enter for both: \").strip()\n",
        "    \n",
        "    if choice == \"1\":\n",
        "        print(\"\\n Running Regular Mobility Pipeline...\")\n",
        "        corpus, base_path = main_with_period_separation()\n",
        "        \n",
        "        # Run analysis\n",
        "        print(\"\\n Running Analysis...\")\n",
        "        patterns, vocabulary = run_analysis_with_period_separation(corpus)\n",
        "        \n",
        "        # Create sample outputs\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"CREATING SAMPLE OUTPUTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        df = load_mobility_datasets()\n",
        "        create_sample_outputs(df, num_users=2)\n",
        "        \n",
        "        # Export additional files\n",
        "        export_for_language_model_training(corpus, \n",
        "            os.path.join(base_path, 'mobility_training_data.jsonl'))\n",
        "        \n",
        "        sequences = export_for_sequence_modeling(corpus)\n",
        "        with open(os.path.join(base_path, 'mobility_sequences.json'), 'w') as f:\n",
        "            json.dump(sequences, f, indent=2)\n",
        "        \n",
        "        bert_data = prepare_for_bert_training(corpus)\n",
        "        with open(os.path.join(base_path, 'bert_training_data.json'), 'w') as f:\n",
        "            json.dump(bert_data, f, indent=2)\n",
        "        \n",
        "        with open(os.path.join(base_path, 'mobility_vocabulary.json'), 'w') as f:\n",
        "            json.dump(vocabulary, f, indent=2)\n",
        "        \n",
        "        print(f\"\\n Regular mobility pipeline completed!\")\n",
        "        \n",
        "    elif choice == \"2\":\n",
        "        print(\"\\n Running POI-Enhanced Mobility Pipeline...\")\n",
        "        enhanced_corpus, poi_vocab, patterns, templates = main_poi_pipeline_with_periods()\n",
        "        \n",
        "        # Run POI analysis\n",
        "        print(\"\\n Running POI Analysis...\")\n",
        "        poi_patterns_analysis, poi_vocabulary_analysis = run_analysis_with_period_separation(enhanced_corpus)\n",
        "        \n",
        "        print(f\"\\n POI-enhanced mobility pipeline completed!\")\n",
        "        \n",
        "    else:  # Default: run both\n",
        "        print(\"\\n Running Regular Mobility Pipeline...\")\n",
        "        corpus, base_path = main_with_period_separation()\n",
        "        \n",
        "        # Run analysis for regular pipeline\n",
        "        print(\"\\n Running Regular Analysis...\")\n",
        "        patterns, vocabulary = run_analysis_with_period_separation(corpus)\n",
        "        \n",
        "        # Export additional files for regular pipeline\n",
        "        export_for_language_model_training(corpus, \n",
        "            os.path.join(base_path, 'mobility_training_data.jsonl'))\n",
        "        \n",
        "        sequences = export_for_sequence_modeling(corpus)\n",
        "        with open(os.path.join(base_path, 'mobility_sequences.json'), 'w') as f:\n",
        "            json.dump(sequences, f, indent=2)\n",
        "        \n",
        "        bert_data = prepare_for_bert_training(corpus)\n",
        "        with open(os.path.join(base_path, 'bert_training_data.json'), 'w') as f:\n",
        "            json.dump(bert_data, f, indent=2)\n",
        "        \n",
        "        with open(os.path.join(base_path, 'mobility_vocabulary.json'), 'w') as f:\n",
        "            json.dump(vocabulary, f, indent=2)\n",
        "        \n",
        "        print(\"\\n Running POI-Enhanced Mobility Pipeline...\")\n",
        "        enhanced_corpus, poi_vocab, poi_patterns, templates = main_poi_pipeline_with_periods()\n",
        "        \n",
        "        # Run POI analysis\n",
        "        print(\"\\n Running POI Analysis...\")\n",
        "        poi_patterns_analysis, poi_vocabulary_analysis = run_analysis_with_period_separation(enhanced_corpus)\n",
        "        \n",
        "        # Create sample outputs\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"CREATING SAMPLE OUTPUTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        df = load_mobility_datasets()\n",
        "        create_sample_outputs(df, num_users=2)\n",
        "        \n",
        "        print(f\"\\n Both pipelines completed!\")\n",
        "    \n",
        "    print(\"\\n ALL DONE!\")\n",
        "    print(\"\\n Generated Files:\")\n",
        "    print(\"  📁 UserMobilityTexts/ (Regular mobility with period separation)\")\n",
        "    print(\"     ├── simple_style/, medium_style/, detailed_style/\")\n",
        "    print(\"     │   ├── normal_period/ (individual user files)\")\n",
        "    print(\"     │   └── emergency_period/ (individual user files)\")\n",
        "    print(\"     ├── user_summaries/ (complete user summaries)\")\n",
        "    print(\"     └── dataset_statistics.txt\")\n",
        "    print(\"  📁 POI_UserMobilityTexts/ (POI-enhanced mobility with period separation)\")\n",
        "    print(\"     ├── normal_period/, emergency_period/ (POI-enhanced user files)\")\n",
        "    print(\"     ├── poi_vocabulary.json, poi_patterns.json\")\n",
        "    print(\"     ├── trajectory_templates.json, synthetic_trajectories.json\")\n",
        "    print(\"     └── bert_poi_training.json, gpt_mobility_training.txt\")\n",
        "    print(\"\\n Each user now has separate files for normal and emergency periods!\")\n",
        "    print(\" Check the user_summaries folder for complete user analysis across both periods!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " Starting Mobility Text Generation with Period Separation...\n\nChoose pipeline:\n1. Regular mobility pipeline\n2. POI-enhanced mobility pipeline\n3. Both pipelines\n\n Running Regular Mobility Pipeline...\nCreating folder structure...\nLoading mobility datasets...\nLoaded 12247358 mobility records for 10000 users\nNormal period: 61 days (0-60)\nEmergency period: 14 days (61-75)\n\nGenerating mobility text corpus by period...\n\nProcessing simple narrative style...\n  Normal period: 10000 users\n  Emergency period: 10000 users\n\nProcessing medium narrative style...\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1754458911414
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}